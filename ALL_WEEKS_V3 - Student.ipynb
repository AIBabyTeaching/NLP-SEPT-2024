{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Sept: 2024\n",
    "\n",
    "In this notebook, we have an end-to-end NLP crash course for september 2024\n",
    "\n",
    "Authors:\n",
    "- Eng. Ahmed Métwalli\n",
    "- Eng. Alia Elhefny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "### 1.1 Introduction to NLP\n",
    "\n",
    "Natural Language Processing (NLP) is a multidisciplinary field that combines computer science, linguistics, and artificial intelligence to enable machines to understand, interpret, and generate human language. NLP is ubiquitous in the modern digital world, powering applications such as voice assistants, translation services, and sentiment analysis.\n",
    "\n",
    "#### 1.1.1 What is NLP in the Real World?\n",
    "\n",
    "NLP is applied in various real-world scenarios, including:\n",
    "\n",
    "- **Text Analysis and Summarization**: Automated generation of concise summaries from large documents.\n",
    "- **Sentiment Analysis**: Assessing the emotional tone of texts in social media or customer reviews.\n",
    "- **Machine Translation**: Translating text from one language to another, as seen in Google Translate.\n",
    "- **Chatbots and Virtual Assistants**: Enabling conversational interfaces in applications like Siri, Alexa, and customer support bots.\n",
    "\n",
    "#### 1.1.2 NLP Tasks\n",
    "\n",
    "Key tasks in NLP include:\n",
    "\n",
    "- **Tokenization**: Splitting text into individual words or phrases.\n",
    "- **Named Entity Recognition (NER)**: Identifying and classifying entities like names, places, and organizations.\n",
    "- **Part-of-Speech (POS) Tagging**: Determining the grammatical category (noun, verb, etc.) of each word.\n",
    "- **Dependency Parsing**: Analyzing the grammatical structure of a sentence.\n",
    "- **Text Classification**: Assigning predefined categories to text data, such as spam detection in emails.\n",
    "- **Sentiment Analysis**: Detecting the sentiment or emotion expressed in text.\n",
    "\n",
    "### 1.2 What is Language?\n",
    "\n",
    "Language is a complex system of communication used by humans, comprising various components that convey meaning and facilitate interaction. It consists of several fundamental building blocks:\n",
    "\n",
    "#### 1.2.1 Building Blocks of Language\n",
    "\n",
    "1. **Phonemes**: The smallest units of sound in a language. For example, the word \"cat\" has three phonemes: /k/, /æ/, and /t/.\n",
    "2. **Morphemes**: The smallest units of meaning. \"Unbelievable\" has three morphemes: \"un-\", \"believe\", and \"-able\".\n",
    "3. **Lexemes**: The set of all inflected forms of a single word. For example, \"run\" includes \"runs\", \"ran\", and \"running\".\n",
    "4. **Syntax**: The arrangement of words and phrases to create well-formed sentences. It governs the grammatical structure of language.\n",
    "5. **Context**: The situational background that influences the meaning of words and sentences.\n",
    "\n",
    "### 1.3 Introduction to Approaches to NLP\n",
    "\n",
    "NLP can be approached using various methods, each with its strengths and limitations:\n",
    "\n",
    "#### 1.3.1 Heuristics-Based NLP\n",
    "\n",
    "- Utilizes rule-based methods to process language.\n",
    "- Effective for well-defined, small-scale problems.\n",
    "- Example: Regular expressions for pattern matching in text.\n",
    "\n",
    "#### 1.3.2 Machine Learning for NLP\n",
    "\n",
    "- Uses statistical methods and algorithms to learn from data.\n",
    "- Techniques include supervised and unsupervised learning.\n",
    "- Common algorithms: Naive Bayes, Support Vector Machines (SVM), and decision trees.\n",
    "\n",
    "#### 1.3.3 Deep Learning for NLP\n",
    "\n",
    "- Leverages neural networks, especially deep neural networks, to model complex language patterns.\n",
    "- Significant advancements have been made with architectures like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers.\n",
    "- Applications include language translation, text generation, and question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1: Environment + Hands-on Regex (Heuristic Based)\n",
    "\n",
    "### Environment Preparation:\n",
    "- Download anaconda: https://www.anaconda.com/download/success\n",
    "- Create a new environment called 'NLP_SEPT_2024'\n",
    "    - Set Python version 3.11.x\n",
    "    - Install:\n",
    "        - Notebook\n",
    "        - JupyterLab\n",
    "        - VS Code\n",
    "        - CMD Prompt\n",
    "        - Powershell Prompt\n",
    "    - In Python install basic packages (pip install `package`):\n",
    "        - pandas\n",
    "        - numpy\n",
    "        - matplotlib\n",
    "        - seaborn\n",
    "        - wordcloud # Visualizing the most frequent words in a corpus.\n",
    "        - nltk # Tokenization, POS tagging, stemming, and more.\n",
    "        - spacy # Named Entity Recognition (NER), dependency parsing, and part-of-speech tagging.\n",
    "        - textblob # Sentiment analysis, translation, and language detection.\n",
    "        - gensim # Topic modeling, document similarity, and word embeddings.\n",
    "        - torch # Implementing custom deep learning architectures, fine-tuning models like BERT for NLP tasks.\n",
    "        - transformers # Text classification, translation, question answering, and language generation using pre-trained models.\n",
    "        - sentence-transformers # Text similarity, clustering, and retrieval tasks.\n",
    "        - tensorflow\n",
    "        - keras\n",
    "        - scikit-learn\n",
    "        - chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Regex Patterns (https://docs.python.org/3/library/re.html):\n",
    "-        . - Matches any character except a newline.\n",
    "-        ^ - Matches the start of the string.\n",
    "-        $ - Matches the end of the string.\n",
    "-        * - Matches 0 or more repetitions of the preceding element.\n",
    "-        + - Matches 1 or more repetitions of the preceding element.\n",
    "-        ? - Matches 0 or 1 repetition of the preceding element.\n",
    "-        {n} - Matches exactly n repetitions of the preceding element.\n",
    "-        {n,} - Matches n or more repetitions of the preceding element.\n",
    "-        {n,m} - Matches between n and m repetitions of the preceding element.\n",
    "-        [] - Matches any one of the enclosed characters.\n",
    "-        | - Alternation; matches either the pattern before or after the |.\n",
    "-        () - Groups multiple patterns into one.\n",
    "-        \\d: Matches any digit (equivalent to [0-9]).\n",
    "-        \\D: Matches any non-digit character.\n",
    "-        \\s: Matches any whitespace character (spaces, tabs, newlines).\n",
    "-        \\S: Matches any non-whitespace character.\n",
    "-        \\b: Matches a word boundary (the position between a word and a non-word character).\n",
    "-        \\B: Matches a non-word boundary.\n",
    "-        \\w: Matches any word character (alphanumeric plus underscore).\n",
    "-        \\W: Matches any non-word character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Practice REGEX: https://regex101.com/r/rsVgaP/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example strings\n",
    "text = \"You should call 911 now. 911 is the emergency number\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all numbers in the text\n",
    "pattern = ...\n",
    "# Use re.findall()\n",
    "matches = ...\n",
    "print(f\"Numbers found: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all numbers with the word '999'\n",
    "pattern = ...\n",
    "# hint use re.sub()\n",
    "replaced_text = ...\n",
    "print(replaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emails</th>\n",
       "      <th>username</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>john.doe</td>\n",
       "      <td>example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jane_smith@abc.co.uk</td>\n",
       "      <td>jane_smith</td>\n",
       "      <td>abc.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invalid.email@com</td>\n",
       "      <td>invalid.email</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 emails       username       domain\n",
       "0  john.doe@example.com       john.doe  example.com\n",
       "1  jane_smith@abc.co.uk     jane_smith    abc.co.uk\n",
       "2     invalid.email@com  invalid.email          NaN"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Email \n",
    "data = {'emails': ['john.doe@example.com', 'jane_smith@abc.co.uk', 'invalid.email@com']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Hint: Use df['col'].str.extract()\n",
    "\n",
    "# Extract the username separately\n",
    "# ^: Start of the string.\n",
    "# ([\\w.%+-]+): Captures the username part of the email.\n",
    "# [\\w.%+-]: Matches any word character (letters, digits, underscores), plus the special characters . % + -.\n",
    "# +: One or more of the preceding characters.\n",
    "# @: Matches the literal @ symbol, which is required to separate the username and domain.\n",
    "df['username'] = ...\n",
    "\n",
    "# Extract the domain separately\n",
    "# @: Matches the literal @ symbol, which precedes the domain part.\n",
    "# ([\\w.-]+\\.[a-zA-Z]{2,}): Captures the domain part of the email.\n",
    "# [\\w.-]+: Matches the main domain part, including letters, digits, hyphens, and dots.\n",
    "# \\.[a-zA-Z]{2,}: Matches the top-level domain (TLD) with at least two letters.\n",
    "# $: End of the string.\n",
    "df['domain'] = ...\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: [True, False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Email Validation\n",
    "def validate_email(email):\n",
    "    pattern = ...\n",
    "    # ^(?!.*\\.\\.): Negative lookahead to ensure there are no consecutive dots in the email string.\n",
    "    # [a-zA-Z0-9._%+-]+: Matches the local part (username) of the email. Allows letters, digits, and special characters such as ., _, %, +, -.\n",
    "    # @[a-zA-Z0-9.-]+: Matches the domain part. Allows letters, digits, hyphens, and dots. This pattern allows a single dot but not consecutive dots within the domain part.\n",
    "    # \\.[a-zA-Z]{2,6}$: Matches the TLD with 2 to 6 alphabetic characters, which covers most common TLDs like .com, .org, .museum, etc.\n",
    "    return bool(re.match(pattern, email))\n",
    "# Test the refined function\n",
    "emails = ['test.email@example.com', 'invalid-email@.com', 'name@domain.co', 'test..email@example.com', 'test@domain.c', 'test@domain.toolongtld']\n",
    "results = [validate_email(email) for email in emails]\n",
    "print(f\"Validation Results: {results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: [True, True, False]\n"
     ]
    }
   ],
   "source": [
    "# Phone Number Validation\n",
    "def validate_phone_number(number):\n",
    "    # Pattern to match common phone number formats\n",
    "    pattern = ...\n",
    "\n",
    "    # ^: Start of the string.\n",
    "    # (\\+\\d{1,3}[-.\\s]?)?: Matches the optional country code part.\n",
    "        # \\+: Matches a literal plus sign '+' at the start, indicating an international code.\n",
    "        # \\d{1,3}: Matches 1 to 3 digits for the country code (e.g., '1' for the US, '44' for the UK).\n",
    "        # [-.\\s]?: Matches an optional separator, which can be a hyphen '-', a dot '.', or a space ' '.\n",
    "        # ?: Makes the entire country code part optional.\n",
    "    # (\\(?\\d{3}\\)?[-.\\s]?)?: Matches the optional area code part.\n",
    "        # \\(?\\d{3}\\)?: Matches 3 digits for the area code, which may or may not be enclosed in parentheses. \n",
    "            # - \\(? : Matches an optional opening parenthesis '('.\n",
    "            # - \\d{3}: Matches exactly 3 digits for the area code.\n",
    "            # - \\)?: Matches an optional closing parenthesis ')'.\n",
    "        # [-.\\s]?: Matches an optional separator (hyphen, dot, or space).\n",
    "            # ?: Makes the entire area code part optional.\n",
    "    # (\\d{3}[-.\\s]?\\d{4}): Matches the main phone number part.\n",
    "        # \\d{3}: Matches exactly 3 digits.\n",
    "        # [-.\\s]?: Matches an optional separator (hyphen, dot, or space).\n",
    "        # \\d{4}: Matches exactly 4 digits for the remaining part of the phone number.\n",
    "    # $: End of the string. Ensures that the pattern matches the entire phone number from start to end.\n",
    "    \n",
    "    return bool(re.fullmatch(pattern, number))\n",
    "\n",
    "\n",
    "# Test the function with a list of phone numbers\n",
    "numbers = ['+1-800-555-5555',  # Valid: Includes country code and separators.\n",
    "           '(123) 456 7890',   # Valid: Area code in parentheses and spaces as separators.\n",
    "           '12345']            # Invalid: Too short to be a valid phone number.\n",
    "\n",
    "# Validate each phone number using the function\n",
    "results = [validate_phone_number(number) for number in numbers]\n",
    "\n",
    "# Display the validation results for each phone number\n",
    "print(f\"Validation Results: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URLs: ['https://www.example.com', 'http://blog.example.com']\n"
     ]
    }
   ],
   "source": [
    "# Extract URLs\n",
    "\n",
    "# Extracting URLs from the given text\n",
    "text = 'Visit our website at https://www.example.com or follow us at http://blog.example.com'\n",
    "\n",
    "# Define the pattern to match URLs\n",
    "pattern = ...\n",
    "\n",
    "# https?://: \n",
    "# - https?: Matches the literal 'http' followed optionally by 's'. This means it can match both 'http' and 'https'.\n",
    "# - ://: Matches the literal characters '://', which are required after 'http' or 'https' in a URL.\n",
    "\n",
    "# [a-zA-Z0-9./-]+:\n",
    "# - [a-zA-Z0-9./-]: Character set that matches any of the following characters:\n",
    "#   - a-z: Lowercase English letters.\n",
    "#   - A-Z: Uppercase English letters.\n",
    "#   - 0-9: Digits.\n",
    "#   - . (dot): Matches the literal dot, which is used in domain names and paths.\n",
    "#   - / (forward slash): Matches the literal slash, which is used to separate different parts of the URL.\n",
    "#   - - (hyphen): Matches the literal hyphen, which can be part of domain names or paths.\n",
    "# - +: Quantifier that matches one or more of the preceding characters in the set, ensuring the pattern matches the entire URL.\n",
    "\n",
    "# Hint: Use re.findall()\n",
    "urls = ...\n",
    "\n",
    "print(f\"Extracted URLs: {urls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Dates: ['23/05/1995', '15-04-1992']\n"
     ]
    }
   ],
   "source": [
    "# Extract Birthday\n",
    "# Sample text containing dates\n",
    "text = \"John's birthday is on 23/05/1995 and Mary's is on 15-04-1992.\"\n",
    "\n",
    "# Define the pattern to match date formats\n",
    "pattern = ...\n",
    "\n",
    "# \\b: Matches a word boundary, ensuring that the pattern matches whole numbers and not parts of larger strings.\n",
    "    # - This prevents partial matches like '123' in '123abc'.\n",
    "# \\d{1,2} or \\d{2,4}: \n",
    "# - \\d: Matches any digit from 0 to 9.\n",
    "# - {1,2}: Matches lower or upper digits for the day or month part, allowing for numbers like '3' or '23'.\n",
    "# [-/]: - Matches either a hyphen '-' or a forward slash '/', which are common separators in date formats.\n",
    "\n",
    "\n",
    "# Hint Use re.findall()\n",
    "dates = ...\n",
    "\n",
    "print(f\"Extracted Dates: {dates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello there', 'How are you today', \"Let's learn regex\"]\n"
     ]
    }
   ],
   "source": [
    "# Splitting text to be split into sentences\n",
    "text = \"Hello there! How are you today? Let's learn regex.\"\n",
    "\n",
    "# Define the pattern to split the text into sentences\n",
    "pattern = ...\n",
    "\n",
    "# Explanation of the pattern:\n",
    "# [.!?]:\n",
    "# - [ ]: Square brackets define a character class, which matches any one of the enclosed characters.\n",
    "# - .: Matches a literal period (.) which marks the end of a sentence.\n",
    "# - !: Matches a literal exclamation mark (!) which marks the end of an exclamatory sentence.\n",
    "# - ?: Matches a literal question mark (?) which marks the end of a question.\n",
    "\n",
    "# Hint: Use re.split(pattern,text)\n",
    "sentences = ...\n",
    "\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "print(f\"Sentences: {sentences}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Approaches & Detailed NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Approaches to NLP\n",
    "\n",
    "### Heuristics-Based NLP\n",
    "Heuristic approaches rely on rules and patterns for natural language processing, without relying on data-driven methods. Commonly, regular expressions (regex) are used.\n",
    "\n",
    "**Example:** Named Entity Recognition (NER) using heuristics might involve defining a set of patterns to detect names, locations, or dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ahmed', 'Egypt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Ahmed lives in Egypt\"\n",
    "pattern = r\"...\"  # Simple heuristic for proper nouns\n",
    "# Use \\b and make sure the first letter is capital and then get the rest of the word\n",
    "entities = re.findall(pattern, text)\n",
    "print(entities)  # ['Ahmed', 'Egypt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning for NLP\n",
    "Machine learning approaches involve training models on labeled data. A basic example is using the Naive Bayes algorithm for text classification. One common method for text representation in machine learning is the bag-of-words model, which transforms text into numerical features by counting the occurrences of words.\n",
    "\n",
    "**Equation:**\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "This is the basis of the Naive Bayes classifier, where:\n",
    "- \\( y \\) is the class label (e.g., positive or negative sentiment),\n",
    "- \\( x \\) is the text feature (e.g., a sequence of words),\n",
    "- \\( P(y|x) \\) is the posterior probability of class \\( y \\) given the text feature \\( x \\),\n",
    "- \\( P(x|y) \\) is the likelihood of text feature \\( x \\) given class \\( y \\),\n",
    "- \\( P(y) \\) is the prior probability of class \\( y \\),\n",
    "- \\( P(x) \\) is the probability of the text feature \\( x \\).\n",
    "\n",
    "**Example:** Text classification using Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Utility:\n",
    "    @staticmethod\n",
    "    def vectorize_and_split(texts,labels):\n",
    "        # Step 1: Vectorizing the text data using CountVectorizer (Bag-of-Words model)\n",
    "        vectorizer = ... # Use count vectorizer\n",
    "        X_v = vectorizer.fit_transform(...)  # Transform text into a matrix of token counts\n",
    "        # Step 2: Splitting data into training and test sets\n",
    "        X_train, ..., ..., y_test = train_test_split(..., labels, test_size=0.2, random_state=42)\n",
    "        return X_train,...,...,y_test,vectorizer\n",
    "    @staticmethod\n",
    "    def model_pipeline(X_train,X_test,y_train,y_test):\n",
    "        # Step 3: Initializing the Naive Bayes model (MultinomialNB is suitable for text classification)\n",
    "        model = ... # Use the multinomial NB\n",
    "        # Step 4: Training the model on the training data\n",
    "        ... # Train the model\n",
    "        # Step 5: Making predictions on the test data\n",
    "        y_pred = ... # Predict\n",
    "        # Step 6: Evaluating the model's performance\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return ...\n",
    "    @staticmethod\n",
    "    def model_predict_and_evaluate(model,new_texts,vectorizer):\n",
    "        X_new = vectorizer.transform(...) # transform new data\n",
    "        predictions = model.predict(...) # predict new data \n",
    "        \n",
    "        # Output predictions\n",
    "        for text, pred in zip(new_texts, predictions):\n",
    "            sentiment = \"Positive\" if pred == 1 else \"Negative/Neutral\"\n",
    "            print(f\"Text: '{text}' => Sentiment: {sentiment}\")\n",
    "\n",
    "class Helper:\n",
    "    @staticmethod\n",
    "    def run_pipeline(new_texts,texts,labels):\n",
    "        X_train,...,...,y_test,vectorizer = Utility.vectorize_and_split(texts=texts,labels=labels)\n",
    "        model = Utility.model_pipeline(X_train=X_train,\n",
    "                                       X_test=...,\n",
    "                                       y_train=...,\n",
    "                                       y_test=y_test)\n",
    "        Utility.model_predict_and_evaluate(model,new_texts,vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Predicting the sentiment of new texts\n",
    "new_texts = [\n",
    "    \"Ahmed enjoys coding in Python\",\n",
    "    \"The service was horrible and I am disappointed\",\n",
    "    \"Quantum mechanics is an intriguing field\",\n",
    "    \"I didn't like the food at all, it was tasteless\"\n",
    "]\n",
    "# Sample dataset with texts and corresponding sentiment labels (1: positive, 0: neutral/negative)\n",
    "texts = [\n",
    "    \"Ahmed loves NLP and enjoys learning about machine learning\",\n",
    "    \"Egypt has beautiful landscapes and rich culture\",\n",
    "    \"Heuristics are simple yet effective in problem solving\",\n",
    "    \"Python is a great language for data science and AI\",\n",
    "    \"Natural language processing is fascinating\",\n",
    "    \"I don't like spam emails\",\n",
    "    \"This movie was very boring and too long\",\n",
    "    \"The weather in Egypt is warm and sunny most of the year\",\n",
    "    \"I had a terrible customer service experience\",\n",
    "    \"AI is transforming industries and creating new opportunities\",\n",
    "    \"This is the worst product I have ever bought\",\n",
    "    \"I love exploring new places and experiencing different cultures\",\n",
    "    \"The food at the restaurant was fantastic\",\n",
    "    \"The traffic in Cairo is terrible during rush hours\",\n",
    "    \"I enjoy spending time with my family on the weekends\",\n",
    "    \"The company's customer service was exceptional\",\n",
    "    \"This is an average book with no exciting plot\",\n",
    "    \"I am fascinated by advancements in quantum computing\",\n",
    "    \"The park near my house is always clean and peaceful\",\n",
    "    \"This phone has terrible battery life, I am disappointed\",\n",
    "    # Additional data:\n",
    "    \"The music was beautiful and uplifting\",\n",
    "    \"I am not happy with the slow internet speed\",\n",
    "    \"The staff at the hotel were very polite and helpful\",\n",
    "    \"This software is extremely buggy and crashes often\",\n",
    "    \"I had a great time at the concert\",\n",
    "    \"I am fed up with all the ads in this app\",\n",
    "    \"The customer support was rude and unhelpful\",\n",
    "    \"The car's performance was beyond my expectations\",\n",
    "    \"I hate waiting in long lines\",\n",
    "    \"The book was interesting but too long\",\n",
    "    \"I had a wonderful vacation with my family\",\n",
    "    \"This smartphone is overpriced and not worth the money\",\n",
    "    \"I enjoyed watching the latest movie at the cinema\",\n",
    "    \"The delivery service was fast and efficient\",\n",
    "    \"This laptop is lightweight and has a long battery life\",\n",
    "    \"I am dissatisfied with the product's quality\",\n",
    "    \"The new restaurant has a cozy atmosphere and delicious food\",\n",
    "    \"The airline lost my luggage and I am very upset\",\n",
    "    \"The presentation was informative and well-organized\",\n",
    "    \"The hotel room was dirty and smelled bad\"\n",
    "]\n",
    "# Labels: 1 for positive sentiment, 0 for neutral or negative sentiment\n",
    "labels = [\n",
    "    1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
    "    # Additional labels:\n",
    "    1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n",
      "Text: 'Ahmed enjoys coding in Python' => Sentiment: Positive\n",
      "Text: 'The service was horrible and I am disappointed' => Sentiment: Negative/Neutral\n",
      "Text: 'Quantum mechanics is an intriguing field' => Sentiment: Positive\n",
      "Text: 'I didn't like the food at all, it was tasteless' => Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "Helper.run_pipeline(new_texts=new_texts,\n",
    "                    texts=texts,\n",
    "                    labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminology about last example!!\n",
    "\n",
    "How CountVectorizer Works:\n",
    "CountVectorizer is a simple and effective tool used in Natural Language Processing (NLP) to convert a collection of text documents into a matrix of token counts. It essentially creates a Bag-of-Words (BoW) representation of the text data, where:\n",
    "\n",
    "- Tokenization: It splits the text into words or tokens.\n",
    "- Vocabulary creation: It builds a vocabulary (unique words from the dataset).\n",
    "- Count representation: Each text document is then represented as a vector of word counts, i.e., how many times each word from the vocabulary appears in each document.\n",
    "\n",
    "---\n",
    "\n",
    "Is CountVectorizer the Best Option?\n",
    "While CountVectorizer is effective in many cases, it has limitations:\n",
    "\n",
    "- Ignores word importance: It treats all words equally, meaning frequent words like \"the\" or \"is\" get as much weight as important words, which may not be desirable.\n",
    "- Doesn't consider word context: It doesn't capture the relationship between words (e.g., \"New York\" is treated as two separate words rather than a single entity).\n",
    "- Sparse representations: For large vocabularies, the resulting matrix can be very sparse, with many zero entries.\n",
    "\n",
    "**Other Options and Their Differences:**\n",
    "\n",
    "**TfidfVectorizer** (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "- What it does: It transforms text into a matrix of TF-IDF features. It takes into account the frequency of a word in a document and its frequency across all documents. This way, less frequent but important words are given higher weight compared to common words.\n",
    "- Why it's better: It reduces the importance of frequently occurring words that may not be informative (like \"the\" or \"is\"), leading to better performance for some tasks.\n",
    "- Example: Words that appear in fewer documents will have higher weight, while common words will have a lower weight.\n",
    "\n",
    "\n",
    "**HashingVectorizer**:\n",
    "\n",
    "- What it does: Similar to CountVectorizer, but instead of building a vocabulary, it uses a hashing trick to map terms to indices in a fixed-size vector space.\n",
    "- Why it's different: No need to store a vocabulary, which saves memory. This can be useful when dealing with very large datasets.\n",
    "\n",
    "- Downside: It's not possible to reverse the transformation and map back to the original words since hashing is a one-way process.\n",
    "\n",
    "**Word2Vec or GloVe (Word Embeddings)**:\n",
    "\n",
    "- What it does: Instead of representing words based on their counts, it captures the semantic meaning of words by embedding them into dense vectors. Similar words are mapped close to each other in the vector space.\n",
    "- Why it's better: It captures semantic relationships between words, so words like \"king\" and \"queen\" would have similar vectors, while traditional vectorizers treat them as completely different.\n",
    "- Downside: More complex to train and use, but it can provide more informative features, especially for tasks that benefit from semantic understanding.\n",
    "\n",
    "\n",
    "**BERT, GPT, and other transformer-based models**:\n",
    "\n",
    "- What they do: These models use deep learning to generate context-aware word embeddings. Each word is represented based on the context in which it appears.\n",
    "- Why it's better: Unlike CountVectorizer or TfidfVectorizer, they capture the meaning of a word depending on its surrounding words, making them much more powerful for complex NLP tasks like sentiment analysis, text classification, and question answering.\n",
    "Downside: Computationally expensive and require large datasets for training. They also need more processing power compared to simpler vectorizers.\n",
    "\n",
    "When to Use Which?\n",
    "- For simple tasks with limited data: CountVectorizer or - TfidfVectorizer are usually sufficient. TfidfVectorizer tends to perform better in cases where the frequency of common words needs to be downplayed.\n",
    "\n",
    "- For large-scale or production tasks: HashingVectorizer is useful when dealing with very large datasets, especially when memory is a concern.\n",
    "\n",
    "- For advanced NLP tasks requiring semantic understanding: Word embeddings (Word2Vec, GloVe) or transformer-based models like BERT are more powerful, capturing context and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Vectorizers and NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|    | Technique         | Features                                                                   | Advantages                                                                                              | Disadvantages                                               | Use case                                                                                                                                                                  | Principle of Work                                                                                         |\n",
      "+====+===================+============================================================================+=========================================================================================================+=============================================================+===========================================================================================================================================================================+===========================================================================================================+\n",
      "|  0 | CountVectorizer   | Simple token counts, creates a Bag-of-Words representation                 | Simple and efficient for small datasets                                                                 | Doesn't capture word importance or relationships            | Best for small datasets or when you need simple frequency counts of words. Example: Spam detection with basic word count analysis.                                        | 1. Tokenization: Splits text into words.                                                                  |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 2. Vocabulary creation: Builds a vocabulary of unique words.                                              |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 3. Count representation: Represents each document as a vector of word counts.                             |\n",
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|  1 | TfidfVectorizer   | Transforms text into TF-IDF matrix, considers word importance              | Reduces importance of common words, better for frequent-infrequent word imbalance                       | Ignores word context, no semantic information               | Ideal for text classification tasks where common words need to be down-weighted. Example: News categorization where frequent words like 'the', 'is' are less informative. | 1. Tokenization: Splits text into words.                                                                  |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 2. Vocabulary creation: Builds a vocabulary.                                                              |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 3. Calculate TF: Term frequency for each word in a document.                                              |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 4. Calculate IDF: Inverse document frequency across documents.                                            |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 5. TF-IDF representation: Multiply TF by IDF for each word.                                               |\n",
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|  2 | HashingVectorizer | Uses hashing trick to map terms to indices, fixed-size vectors             | Memory-efficient for large datasets, no need to store vocabulary                                        | Cannot reverse mapping, no interpretability                 | Efficient for extremely large datasets or real-time systems. Example: Large-scale text search systems where memory is a concern.                                          | 1. Tokenization: Splits text into words.                                                                  |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 2. Hashing: Uses a hash function to map each word to a fixed vector space.                                |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 3. Count representation: Words mapped to hashed vector space are counted.                                 |\n",
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|  3 | Word2Vec          | Embeds words into dense vectors, captures semantic meaning                 | Captures semantic meaning, useful for context-based tasks                                               | Requires training, more complex to use                      | Useful for tasks that need semantic similarity between words. Example: Sentiment analysis where similar words ('good', 'great') should have close representations.        | 1. Tokenization: Splits text into words.                                                                  |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 2. Training: Learns dense vector representation of words based on word co-occurrence in the text.         |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 3. Embedding: Represents each word as a vector capturing its meaning.                                     |\n",
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|  4 | Doc2Vec           | Embeds entire documents or sentences into dense vectors, capturing context | Captures document-level semantics, useful for tasks requiring sentence or paragraph-level understanding | Requires training, complex to use for small datasets        | Best for tasks where you need sentence or document-level embeddings. Example: Document classification or summarization tasks.                                             | 1. Tokenization: Splits text into words.                                                                  |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 2. Training: Learns dense vector representation of entire documents based on word context.                |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 3. Embedding: Represents entire documents as vectors, capturing semantics.                                |\n",
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|  5 | GloVe             | Similar to Word2Vec, pretrained on co-occurrence statistics                | Captures global co-occurrence statistics for better word representation                                 | Requires large corpus for training, more complex            | Good for transfer learning tasks, pretrained on large corpora. Example: Named entity recognition (NER) using pretrained word embeddings.                                  | 1. Co-occurrence matrix: Builds a matrix of word co-occurrence statistics.                                |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 2. Factorization: Decomposes the matrix to derive dense word vectors.                                     |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 3. Embedding: Represents words as dense vectors based on global statistics.                               |\n",
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|  6 | BERT/GPT          | Context-aware embeddings using deep learning                               | Captures meaning based on word context, highly accurate for complex tasks                               | Computationally expensive, requires large datasets and GPUs | Best for advanced NLP tasks like question answering and sentiment analysis. Example: Sentiment analysis using BERT for highly contextual understanding of text.           | 1. Tokenization: Splits text into tokens (words).                                                         |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 2. Embedding: Creates context-aware embeddings for each word.                                             |\n",
      "|    |                   |                                                                            |                                                                                                         |                                                             |                                                                                                                                                                           | 3. Transformer-based architecture: Uses self-attention mechanism to understand word context in sentences. |\n",
      "+----+-------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Display All Vectorizers with their differences\n",
    "import pandas as pd\n",
    "# Load the CSV file into a pandas dataframe\n",
    "df = pd.read_csv(\"NLP_Vectorization_Techniques__Simplest_to_Most_Complex_.csv\")\n",
    "from tabulate import tabulate\n",
    "# Generate the markdown table using tabulate\n",
    "markdown_table = tabulate(df, headers='keys',tablefmt= 'grid')\n",
    "# Print the markdown table\n",
    "print(markdown_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each vectorizer in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Utility:\n",
    "    \n",
    "    \"\"\"Utility Class:\n",
    "        Tokenization Method: Splits each sentence into a list of words (used for Word2Vec and Doc2Vec).\n",
    "        \n",
    "        count_vectorizer(): Converts the text into a matrix of word counts using CountVectorizer. It outputs a vocabulary (dictionary of words and their indices) and a matrix where each row represents a document, and each column represents the count of a word in the document.\n",
    "        \n",
    "        tfidf_vectorizer(): Converts the text into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) values. Words that occur less frequently but are important receive higher weight, while common words receive lower weight.\n",
    "        \n",
    "        hashing_vectorizer(): Uses the HashingVectorizer to transform the text into a fixed-size vector space without storing the vocabulary. Each word is hashed into one of the predefined dimensions.\n",
    "        \n",
    "        word2vec(): Trains a Word2Vec model to generate a dense vector for each word, capturing semantic relationships. For example, words like \"learning\" and \"education\" would have similar vector representations.\n",
    "        \n",
    "        doc2vec(): Trains a Doc2Vec model to generate document-level embeddings. Each document is represented as a dense vector capturing its meaning as a whole.\n",
    "        \n",
    "        bert_vectorizer(): Uses BERT (Bidirectional Encoder Representations from Transformers) to generate context-aware embeddings for each word in the sentence.\"\"\"\n",
    "        \n",
    "    def tokenize_texts(texts):\n",
    "        \"\"\"Tokenizes the dataset for Word2Vec and Doc2Vec.\"\"\"\n",
    "        return [text.split() for text in texts]\n",
    "\n",
    "    def count_vectorizer(texts):\n",
    "        \"\"\"\n",
    "        Applies CountVectorizer and returns vocabulary and feature matrix.\n",
    "        \n",
    "        Expected output:\n",
    "        Vocabulary: {'machine': 4, 'learning': 3, ...}\n",
    "        Matrix: Shows the count of each word in each document.\n",
    "        Example:\n",
    "        [[0 0 0 1 1 ...]\n",
    "        [0 1 1 0 0 ...]]\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer()\n",
    "        matrix = vectorizer.fit_transform(texts).toarray()\n",
    "        return vectorizer.vocabulary_, matrix\n",
    "\n",
    "    def tfidf_vectorizer(texts):\n",
    "        \"\"\"\n",
    "        Applies TfidfVectorizer and returns vocabulary and feature matrix.\n",
    "        \n",
    "        Expected output:\n",
    "        Vocabulary: {'machine': 4, 'learning': 3, ...}\n",
    "        Matrix: Shows TF-IDF values for each word in each document.\n",
    "        Example:\n",
    "        [[0 0 0 0.45 0.35 ...]\n",
    "        [0 0.34 0.56 0 0 ...]]\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        matrix = vectorizer.fit_transform(texts).toarray()\n",
    "        return vectorizer.vocabulary_, matrix\n",
    "\n",
    "    def hashing_vectorizer(texts):\n",
    "        \"\"\"\n",
    "        Applies HashingVectorizer and returns the feature matrix.\n",
    "        \n",
    "        Expected output:\n",
    "        No vocabulary (since it's hashed).\n",
    "        Matrix: A fixed-size vector space with hashed word counts.\n",
    "        Example:\n",
    "        [[ 0.   0.   0.23  0.33 ...]\n",
    "        [ 0.12 -0.12  0.5  0.1 ...]]\n",
    "        \"\"\"\n",
    "        vectorizer = HashingVectorizer(n_features=10)\n",
    "        HashingVectorizer()\n",
    "        matrix = vectorizer.transform(texts).toarray()\n",
    "        return matrix\n",
    "\n",
    "    def word2vec(texts):\n",
    "        \"\"\"\n",
    "        Trains Word2Vec on the tokenized dataset and returns a vector for 'learning'.\n",
    "        \n",
    "        Expected output:\n",
    "        A dense vector for the word \"learning\", capturing its semantic meaning.\n",
    "        Example:\n",
    "        Word2Vec Vector for 'learning': [0.12, 0.23, -0.45, ...]\n",
    "        \"\"\"\n",
    "        tokenized_texts = Utility.tokenize_texts(texts)\n",
    "        model = Word2Vec(sentences=tokenized_texts, vector_size=5, window=2, min_count=1, workers=4)\n",
    "        return model.wv['learning']\n",
    "\n",
    "    def doc2vec(texts):\n",
    "        \"\"\"\n",
    "        Trains Doc2Vec on the tagged dataset and returns the document vector for document 0.\n",
    "        \n",
    "        Expected output:\n",
    "        A dense vector representing the entire first document.\n",
    "        Example:\n",
    "        Doc2Vec Vector for Document 0: [0.23, -0.14, 0.65, ...]\n",
    "        \"\"\"\n",
    "        tokenized_texts = Utility.tokenize_texts(texts)\n",
    "        tagged_data = [TaggedDocument(words=text, tags=[str(i)]) for i, text in enumerate(tokenized_texts)]\n",
    "        model = Doc2Vec(tagged_data, vector_size=5, window=2, min_count=1, workers=4)\n",
    "        return model.dv['0']\n",
    "\n",
    "    def bert_vectorizer(text):\n",
    "        \"\"\"\n",
    "        Uses BERT to get contextual embeddings for the input text.\n",
    "        \n",
    "        Expected output:\n",
    "        Contextual embeddings for the input sentence, capturing the context of each word.\n",
    "        Example:\n",
    "        BERT Embeddings for 'Machine learning is fun': [0.14, -0.25, 0.35, ...]\n",
    "        \"\"\"\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    # Display helper\n",
    "    def display_result(vectorizer_name, vocabulary, matrix):\n",
    "        print(f\"\\n{vectorizer_name}:\")\n",
    "        print(\"Vocabulary:\", vocabulary)\n",
    "        print(\"Matrix:\\n\", matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper:\n",
    "    def display_all_vectorizers():\n",
    "        vocab_count, count_matrix = Utility.count_vectorizer(texts)\n",
    "        Utility.display_result(\"CountVectorizer\", vocab_count, count_matrix)\n",
    "\n",
    "        vocab_tfidf, tfidf_matrix = Utility.tfidf_vectorizer(texts)\n",
    "        Utility.display_result(\"TfidfVectorizer\", vocab_tfidf, tfidf_matrix)\n",
    "\n",
    "        hashing_matrix = Utility.hashing_vectorizer(texts)\n",
    "        Utility.display_result(\"HashingVectorizer\", None, hashing_matrix)\n",
    "        \n",
    "        word2vec_vector = Utility.word2vec(texts)\n",
    "        \n",
    "        print(\"\\nWord2Vec Vector for 'learning':\", word2vec_vector)\n",
    "\n",
    "        doc2vec_vector = Utility.doc2vec(texts)\n",
    "        print(\"\\nDoc2Vec Vector for Document 0:\", doc2vec_vector)\n",
    "\n",
    "        bert_vector = Utility.bert_vectorizer(\"Machine learning is fun\")\n",
    "        print(\"\\nBERT Embeddings for 'Machine learning is fun':\\n\", bert_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CountVectorizer:\n",
      "Vocabulary: {'machine': 17, 'learning': 16, 'is': 14, 'fun': 11, 'natural': 18, 'language': 15, 'processing': 25, 'includes': 12, 'syntax': 28, 'and': 2, 'semantics': 27, 'deep': 8, 'enables': 9, 'complex': 5, 'pattern': 24, 'recognition': 26, 'artificial': 4, 'intelligence': 13, 'transforming': 31, 'the': 30, 'world': 32, 'text': 29, 'analysis': 1, 'an': 0, 'essential': 10, 'part': 23, 'of': 22, 'nlp': 21, 'neural': 20, 'networks': 19, 'are': 3, 'core': 7, 'component': 6}\n",
      "Matrix:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "TfidfVectorizer:\n",
      "Vocabulary: {'machine': 17, 'learning': 16, 'is': 14, 'fun': 11, 'natural': 18, 'language': 15, 'processing': 25, 'includes': 12, 'syntax': 28, 'and': 2, 'semantics': 27, 'deep': 8, 'enables': 9, 'complex': 5, 'pattern': 24, 'recognition': 26, 'artificial': 4, 'intelligence': 13, 'transforming': 31, 'the': 30, 'world': 32, 'text': 29, 'analysis': 1, 'an': 0, 'essential': 10, 'part': 23, 'of': 22, 'nlp': 21, 'neural': 20, 'networks': 19, 'are': 3, 'core': 7, 'component': 6}\n",
      "Matrix:\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.58137639\n",
      "  0.         0.         0.40249409 0.         0.40249409 0.58137639\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.37796447 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.37796447 0.         0.\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.44057909\n",
      "  0.         0.         0.36128115 0.44057909 0.         0.\n",
      "  0.         0.         0.         0.         0.30501837 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.44057909 0.         0.44057909 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.42720625 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.42720625 0.29576019 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.42720625 0.42720625 0.42720625]\n",
      " [0.37393382 0.37393382 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37393382 0.\n",
      "  0.         0.         0.25887903 0.         0.         0.\n",
      "  0.         0.         0.         0.37393382 0.30663108 0.37393382\n",
      "  0.         0.         0.         0.         0.         0.37393382\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.38280352 0.         0.\n",
      "  0.38280352 0.38280352 0.31390437 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26501964 0.\n",
      "  0.         0.38280352 0.38280352 0.         0.31390437 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "\n",
      "HashingVectorizer:\n",
      "Vocabulary: None\n",
      "Matrix:\n",
      " [[ 0.          0.          0.5         0.         -0.5        -0.5\n",
      "   0.          0.5         0.          0.        ]\n",
      " [ 0.33333333  0.33333333 -0.33333333  0.         -0.33333333 -0.66666667\n",
      "   0.          0.          0.         -0.33333333]\n",
      " [ 0.          0.          0.35355339  0.         -0.35355339  0.\n",
      "   0.         -0.35355339  0.70710678  0.35355339]\n",
      " [ 0.          0.35355339  0.          0.35355339  0.          0.\n",
      "   0.          0.70710678 -0.35355339 -0.35355339]\n",
      " [ 0.28867513 -0.28867513 -0.28867513  0.          0.          0.\n",
      "   0.57735027  0.57735027  0.28867513  0.        ]\n",
      " [ 0.          0.          0.28867513  0.         -0.28867513  0.\n",
      "   0.8660254   0.          0.          0.28867513]]\n",
      "\n",
      "Word2Vec Vector for 'learning': [-0.01059619  0.00473791  0.10198051  0.18016596 -0.18609913]\n",
      "\n",
      "Doc2Vec Vector for Document 0: [-0.10484383 -0.11971834 -0.1979842   0.1712388   0.07125482]\n",
      "\n",
      "BERT Embeddings for 'Machine learning is fun':\n",
      " tensor([[[-0.0445,  0.0230, -0.1621,  ..., -0.2820,  0.0384,  0.7282],\n",
      "         [ 0.1286,  0.1556, -0.2178,  ...,  0.0835,  0.4772,  0.4959],\n",
      "         [-0.4269,  0.1303, -0.0558,  ..., -0.9312,  0.0377,  0.3362],\n",
      "         [-0.2924, -0.1132,  0.2181,  ...,  0.0562, -0.0214,  0.7718],\n",
      "         [-0.5572, -0.8273, -0.2100,  ...,  0.4853,  0.2728,  0.3940],\n",
      "         [ 0.7426,  0.0573, -0.3088,  ...,  0.1753, -0.8196,  0.0047]]])\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "texts = [\n",
    "    \"Machine learning is fun\",\n",
    "    \"Natural language processing includes syntax and semantics\",\n",
    "    \"Deep learning enables complex pattern recognition\",\n",
    "    \"Artificial intelligence is transforming the world\",\n",
    "    \"Text analysis is an essential part of NLP\",\n",
    "    \"Neural networks are a core component of deep learning\"\n",
    "]\n",
    "Helper.display_all_vectorizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "CountVectorizer:\n",
    "Vocabulary: Displays the mapping of each word to a unique index.\n",
    "Matrix: Shows the count of each word in each document. For example, the word \"learning\" occurs once in the first and third sentences, while \"fun\" occurs once in the first.\n",
    "\n",
    "TfidfVectorizer:\n",
    "Vocabulary: Same as CountVectorizer, but now the matrix contains TF-IDF values. Words that appear frequently across documents receive lower weight (like \"is\"), while rare words receive higher weight (like \"semantics\").\n",
    "Matrix: Contains floating-point numbers representing the relative importance of each word in a document. For example, words like \"fun\" and \"machine\" receive higher values because they are less frequent but important.\n",
    "\n",
    "HashingVectorizer:\n",
    "Vocabulary: Not available (since it uses a hash function).\n",
    "Matrix: Each document is represented by a fixed-size vector. This method is more memory-efficient but does not provide a direct way to map the indices back to the original words.\n",
    "\n",
    "Word2Vec:\n",
    "Vector: For the word \"learning\", a dense vector is produced. These vectors are continuous-valued representations that capture the semantic meaning of words. Words with similar meanings will have similar vectors.\n",
    "\n",
    "Doc2Vec:\n",
    "Vector: Each document is represented as a dense vector. This vector captures the overall semantics of the document. In this case, document 0 (\"Machine learning is fun\") is represented by a vector that encodes its meaning as a whole.\n",
    "\n",
    "BERT:\n",
    "Embeddings: Each word in the sentence \"Machine learning is fun\" is represented by a contextualized embedding. The meaning of each word depends on the context of the other words in the sentence, making it powerful for tasks like question answering, sentiment analysis, etc. For example, \"learning\" in \"machine learning\" has a different meaning than in \"learning from mistakes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for NLP\n",
    "Deep learning approaches, such as Recurrent Neural Networks (RNNs) and Transformers, have revolutionized NLP by modeling complex dependencies in text data. RNNs use sequential data with hidden states, while Transformers, like BERT, use attention mechanisms for parallel processing.\n",
    "\n",
    "**Equation:** \n",
    "The attention mechanism in Transformers is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Q \\) is the **query**,\n",
    "- \\( K \\) is the **key**,\n",
    "- \\( V \\) is the **value**,\n",
    "\n",
    "and \\( d_k \\) is the dimension of the key.\n",
    "\n",
    "**Example:** Using Huggingface's BERT for text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ...\n",
    "\"\"\"Imports the pipeline function from the Hugging Face Transformers library. \n",
    "The pipeline is a high-level abstraction that allows you to easily use state-of-the-art NLP models without worrying about the underlying complexity.\"\"\"\n",
    "\n",
    "\n",
    "classifier = ...(...) # use sentiment analysis from pipeline\n",
    "\"\"\" Uses Bidirectional Encoder Representations from Transformers (BERT).\n",
    "This should creates a pre-configured pipeline for sentiment analysis.\n",
    "By default, Hugging Face uses a pre-trained model from the BERT family, specifically DistilBERT, which has been fine-tuned for sentiment analysis. It will automatically download the required model weights and tokenizer when you first run the code.\"\"\"\n",
    "\n",
    "\n",
    "result = ...(\"Ahmed enjoys studying NLP\")\n",
    "\"\"\"Text tokenization: Behind the scenes, the input text is first tokenized, which means it's split into smaller units (tokens) that the model can process.\n",
    "Model prediction: The pre-trained model then processes these tokens and predicts whether the text expresses a positive or negative sentiment. Since the text is about \"enjoying\" something, the model will likely predict positive sentiment.\n",
    "Post-processing: The result is then converted back into a human-readable label, which in this case is either \"POSITIVE\" or \"NEGATIVE\", along with a confidence score.\"\"\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Pipeline in Detail\n",
    "\n",
    "### 1. Data Acquisition\n",
    "The first step in any NLP project is acquiring the data. This can come from web scraping, APIs, or existing datasets.\n",
    "\n",
    "**Example:** Load a dataset (e.g., IMDB movie reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "df = pd.DataFrame({'text': data.data, 'label': data.target})\n",
    "\n",
    "# Take a look at the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Cleaning\n",
    "Text cleaning involves removing noise such as punctuation, numbers, stopwords, and converting text to lowercase.\n",
    "\n",
    "**Example:** Simple text cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Metwalli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\"\"\"nltk.corpus.stopwords: provides a corpus of stopwords (common words like \"the\", \"is\", \"in\", etc.) for various languages. Stopwords are often removed in text preprocessing since they don't contribute much to the meaning of the text for certain tasks like classification.\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Purpose: Text cleaning and normalization are crucial steps before further processing or analysis, especially in Natural Language Processing (NLP). This ensures the text is free from unnecessary noise like digits, punctuation, and stopwords, and that it’s all in a consistent format (lowercase).\"\"\"\n",
    "    #Hint: Use .sub() from regex and remove numbers and punctuation\n",
    "    text = ... # This removes all numbers from the text \n",
    "    text = ... # regular expression that matches any character that is not a word (\\w) or whitespace (\\s), effectively removing punctuation.\n",
    "    text = ... #Converts the text to lowercase to standardize it.\n",
    "    text = ' '.join([word for word in text.split() if word not in ...('english')]) # Remove stop words\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-processing\n",
    "Pre-processing includes tokenization, lemmatization, and stemming.\n",
    "\n",
    "**Equation:** For word embeddings like TF-IDF, the formula is:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\log \\left( \\frac{N}{\\text{df}(t)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **tf(t, d)** is the term frequency of term \\( t \\) in document \\( d \\),\n",
    "- **df(t)** is the document frequency of term \\( t \\) across all documents,\n",
    "- **N** is the total number of documents.\n",
    "\n",
    "**Example:** Tokenization and stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Metwalli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Metwalli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#We import these functions because we are going to tokenize the text (split it into individual words) and then apply stemming to convert words to their root form for further processing.\n",
    "#  (e.g., \"running\" vs. \"run\") \n",
    "ps = ... #This creates an instance of the PorterStemmer, which will be used to stem each tokenized word.\n",
    "\n",
    "df['tokens'] = df['cleaned_text'].apply(lambda x: ...) # Use lambda function and .word_tokenize\n",
    "df['stemmed_tokens'] = df['tokens'].apply(lambda x: [... for word in x]) # use .stem(words) from ps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering\n",
    "This involves representing text as numeric features. Popular techniques include Bag-of-Words, TF-IDF, and Word Embeddings (Word2Vec, GloVe).\n",
    "\n",
    "**Example:** TF-IDF vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\"\"\"The TfidfVectorizer is used to convert text into a numerical form that machine learning models can understand. The vectorizer automatically handles tokenization and computes the TF-IDF score for each word.\"\"\"\n",
    "tfidf = ... # use an instance of tfidf\n",
    "X = ... # use Fit & transform in tfidf on the cleaned_text from the df\n",
    "\n",
    "\"\"\"it will:\n",
    "Tokenize the text (i.e., split it into words).\n",
    "Compute the term frequency for each word in each document.\n",
    "Compute the inverse document frequency (IDF) across the corpus.\n",
    "Multiply the TF and IDF values to get the TF-IDF score for each word in each document.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modeling\n",
    "You can now apply machine learning or deep learning models like Logistic Regression, SVM, or neural networks.\n",
    "\n",
    "**Example:** Logistic Regression for text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation\n",
    "Model evaluation metrics include accuracy, precision, recall, and F1 score.\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP** = true positives,\n",
    "- **FP** = false positives,\n",
    "- **FN** = false negatives.\n",
    "\n",
    "**Example:** Evaluating model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       480\n",
      "           1       0.96      0.97      0.97       584\n",
      "           2       0.95      0.96      0.95       591\n",
      "           3       0.93      0.95      0.94       590\n",
      "           4       0.98      0.98      0.98       578\n",
      "           5       0.98      0.98      0.98       593\n",
      "           6       0.94      0.96      0.95       585\n",
      "           7       0.98      0.98      0.98       594\n",
      "           8       1.00      0.99      0.99       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      0.98      0.99       595\n",
      "          12       0.98      0.97      0.98       591\n",
      "          13       1.00      0.99      0.99       594\n",
      "          14       1.00      0.99      1.00       593\n",
      "          15       0.96      1.00      0.98       599\n",
      "          16       0.98      0.99      0.99       546\n",
      "          17       1.00      0.99      1.00       564\n",
      "          18       0.99      0.98      0.98       465\n",
      "          19       0.99      0.92      0.95       377\n",
      "\n",
      "    accuracy                           0.98     11314\n",
      "   macro avg       0.98      0.98      0.98     11314\n",
      "weighted avg       0.98      0.98      0.98     11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(classification_report(df['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Deployment\n",
    "Deploying the model can involve using platforms like FastAPI or Flask for serving predictions.\n",
    "\n",
    "**Example:** FastAPI skeleton for deploying an NLP model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "\n",
    "app = FastAPI()\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "def predict(text: str):\n",
    "    return {\"prediction\": model.predict([text])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Monitoring and Model Updating\n",
    "Monitoring involves tracking model performance post-deployment and updating the model when performance degrades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Dive Deeper into Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevator Pitch (Seamless Intro):\n",
    "NER: Identifies and classifies entities in text (e.g., names, locations, dates) using machine learning or rule-based algorithms.\n",
    "\n",
    "Word2Vec: Creates dense word embeddings that capture semantic similarity between words by positioning similar words close in a vector space.\n",
    "\n",
    "Difference: NER is for entity classification, while Word2Vec is for semantic word relationships.\n",
    "\n",
    "Combination: Using Word2Vec embeddings can improve NER by providing better context understanding for entity recognition and classification.\n",
    "\n",
    "Notes: Mathematical representation of word is the vector! The features in this example are [authority, have tail, rich, male]. Surrounding words are the context.\n",
    "> Example the word King: King has authority, king does not have tail, king is rich, king gender is male\n",
    ">> Authority: 1, Has tail = 0, Rich = 1, Gender = -1\n",
    "> Example the word man: \n",
    ">> Authority: 0.2, Has tail = 0, Rich = 0.3, Gender = -1\n",
    "\n",
    "### 1. Introduction to Word Embeddings\n",
    "\n",
    "In natural language processing (NLP), understanding the relationships between words is crucial. Traditionally, words were represented as sparse vectors using techniques like one-hot encoding, which lacked semantic meaning. **Word2Vec**, developed by Google, addresses this by creating dense vector representations (embeddings) where similar words have similar vectors, effectively capturing the semantic meaning of words.\n",
    "\n",
    "### 2. Use Cases for Word2Vec\n",
    "\n",
    "Word2Vec’s dense embeddings are powerful for many applications in NLP:\n",
    "\n",
    "- **Sentiment Analysis**: Words with similar sentiments (e.g., \"good\" and \"great\") have similar vector representations.\n",
    "- **Recommendation Systems**: Identifying item similarities based on descriptions or reviews.\n",
    "- **Semantic Search**: Using vector similarity to match contextually similar phrases or words.\n",
    "\n",
    "These embeddings allow algorithms to measure similarity between words, which helps in clustering, classification, and many downstream NLP tasks.\n",
    "\n",
    "### 3. How Word2Vec Works: Models and Training\n",
    "\n",
    "Word2Vec learns word representations through a shallow neural network. It has two main models:\n",
    "\n",
    "1. **Continuous Bag of Words (CBOW)**: Predicts a word based on its surrounding words (context).\n",
    "2. **Skip-Gram**: Predicts surrounding context words given a single word.\n",
    "\n",
    "These models learn to map words to a high-dimensional space where semantically related words are closer to each other.\n",
    "\n",
    "### 4. Skip-Gram Model\n",
    "\n",
    "The **Skip-Gram model** is useful when we want to understand the broader context of a word. For example, given a target word \\( w_t \\), it tries to predict words within a context window of size \\( c \\) around \\( w_t \\).\n",
    "\n",
    "#### Objective Function for Skip-Gram\n",
    "\n",
    "For a given target word \\( w_t \\) and its context words \\( w_{t-c}, \\dots, w_{t+c} \\), the objective is to maximize the log probability:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P(w_{t+j} | w_t)\n",
    "$$\n",
    "\n",
    "where \\( T \\) is the total number of words in the corpus.\n",
    "\n",
    "This probability \\( P(w_{t+j} | w_t) \\) is computed with a softmax function:\n",
    "\n",
    "$$\n",
    "P(w_O | w_I) = \\frac{\\exp(v'_{w_O} \\cdot v_{w_I})}{\\sum_{w=1}^{W} \\exp(v'_w \\cdot v_{w_I})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( v_{w_I} \\) is the input vector for the word \\( w_I \\),\n",
    "- \\( v'_{w_O} \\) is the output vector for the word \\( w_O \\), and\n",
    "- \\( W \\) is the vocabulary size.\n",
    "\n",
    "### 5. Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "In contrast to Skip-Gram, **CBOW** predicts a target word \\( w_t \\) based on surrounding context words. CBOW is often faster and more accurate for frequent words.\n",
    "\n",
    "#### Objective Function for CBOW\n",
    "\n",
    "The objective function for CBOW is to maximize the log probability of predicting the target word given its context:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T \\log P(w_t | w_{t-c}, \\dots, w_{t+c})\n",
    "$$\n",
    "\n",
    "CBOW captures context effectively and is computationally more efficient for larger datasets.\n",
    "\n",
    "### 6. Optimization Techniques: Negative Sampling and Hierarchical Softmax\n",
    "\n",
    "Training with a large vocabulary can be computationally expensive. Word2Vec employs two key techniques to optimize this:\n",
    "\n",
    "- **Negative Sampling**: Rather than updating weights for every word, it updates only for a few sampled \"negative\" words.\n",
    "  \n",
    "  The negative sampling objective is:\n",
    "\n",
    "  $$\n",
    "  \\log \\sigma(v'_{w_O} \\cdot v_{w_I}) + \\sum_{i=1}^k \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma(-v'_{w_i} \\cdot v_{w_I}) \\right]\n",
    "  $$\n",
    "\n",
    "  where \\( \\sigma(x) \\) is the sigmoid function and \\( P_n(w) \\) is the distribution for sampling.\n",
    "\n",
    "- **Hierarchical Softmax**: Uses a binary tree structure to reduce complexity when calculating softmax probabilities, beneficial for large vocabularies.\n",
    "\n",
    "### 7. Limitations of Word2Vec\n",
    "\n",
    "While Word2Vec is powerful, it has some limitations:\n",
    "\n",
    "- **Lacks Contextuality**: Each word has a single representation, regardless of the context in which it appears. This limits its ability to handle words with multiple meanings (polysemy).\n",
    "- **Static Embeddings**: The embeddings are static, meaning they do not dynamically adapt based on sentence context.\n",
    "- **Memory Intensive**: For large vocabularies, Word2Vec requires significant memory to store embeddings.\n",
    "\n",
    "### 8. Example of Word2Vec in Action\n",
    "\n",
    "Suppose we have a sentence: \"The cat sits on the mat.\" Using a window size of 2, the Skip-Gram model might generate training pairs like:\n",
    "\n",
    "- (\"cat\", \"the\"), (\"cat\", \"sits\"), (\"sits\", \"cat\"), (\"sits\", \"on\")\n",
    "\n",
    "The model will learn vectors for \"cat,\" \"sits,\" \"on,\" etc., so that contextually similar words (e.g., \"cat\" and \"dog\") end up with similar embeddings, capturing semantic relationships in the data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on Word2Vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset can be found: http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, shutil, os\n",
    "\n",
    "# File to decompress\n",
    "input_file = 'reviews_Cell_Phones_and_Accessories_5.json.gz'\n",
    "output_file = 'reviews_Cell_Phones_and_Accessories_5.json'\n",
    "\n",
    "# Open the .gz file and write the uncompressed data to the output file\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One line will be a whole json string\n",
    "df = pd.read_json('reviews_Cell_Phones_and_Accessories_5.json', lines=True) # Lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A30TL5EWN6DFXT</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>christina</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>They look good and stick good! I just don't li...</td>\n",
       "      <td>4</td>\n",
       "      <td>Looks Good</td>\n",
       "      <td>1400630400</td>\n",
       "      <td>05 21, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASY55RVNIL0UD</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>emily l.</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These stickers work like the review says they ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Really great product.</td>\n",
       "      <td>1389657600</td>\n",
       "      <td>01 14, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2TMXE2AFO7ONB</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>Erica</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These are awesome and make my phone look so st...</td>\n",
       "      <td>5</td>\n",
       "      <td>LOVE LOVE LOVE</td>\n",
       "      <td>1403740800</td>\n",
       "      <td>06 26, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWJ0WZQYMYFQ4</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>JM</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>Item arrived in great time and was in perfect ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Cute!</td>\n",
       "      <td>1382313600</td>\n",
       "      <td>10 21, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATX7CZYFXI1KW</td>\n",
       "      <td>120401325X</td>\n",
       "      <td>patrice m rogoza</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>awesome! stays on, and looks great. can be use...</td>\n",
       "      <td>5</td>\n",
       "      <td>leopard home button sticker for iphone 4s</td>\n",
       "      <td>1359849600</td>\n",
       "      <td>02 3, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin      reviewerName helpful  \\\n",
       "0  A30TL5EWN6DFXT  120401325X         christina  [0, 0]   \n",
       "1   ASY55RVNIL0UD  120401325X          emily l.  [0, 0]   \n",
       "2  A2TMXE2AFO7ONB  120401325X             Erica  [0, 0]   \n",
       "3   AWJ0WZQYMYFQ4  120401325X                JM  [4, 4]   \n",
       "4   ATX7CZYFXI1KW  120401325X  patrice m rogoza  [2, 3]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  They look good and stick good! I just don't li...        4   \n",
       "1  These stickers work like the review says they ...        5   \n",
       "2  These are awesome and make my phone look so st...        5   \n",
       "3  Item arrived in great time and was in perfect ...        4   \n",
       "4  awesome! stays on, and looks great. can be use...        5   \n",
       "\n",
       "                                     summary  unixReviewTime   reviewTime  \n",
       "0                                 Looks Good      1400630400  05 21, 2014  \n",
       "1                      Really great product.      1389657600  01 14, 2014  \n",
       "2                             LOVE LOVE LOVE      1403740800  06 26, 2014  \n",
       "3                                      Cute!      1382313600  10 21, 2013  \n",
       "4  leopard home button sticker for iphone 4s      1359849600   02 3, 2013  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.shape[0]] = \"Laith is wearing hat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194441, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: Training a word2vec neural network using only the review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reviewText[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gensim has some utils that can be used to remove punctuations and making the text as lower and removing special characters etc .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they',\n",
       " 'look',\n",
       " 'good',\n",
       " 'and',\n",
       " 'stick',\n",
       " 'good',\n",
       " 'just',\n",
       " 'don',\n",
       " 'like',\n",
       " 'the',\n",
       " 'rounded',\n",
       " 'shape',\n",
       " 'because',\n",
       " 'was',\n",
       " 'always',\n",
       " 'bumping',\n",
       " 'it',\n",
       " 'and',\n",
       " 'siri',\n",
       " 'kept',\n",
       " 'popping',\n",
       " 'up',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'irritating',\n",
       " 'just',\n",
       " 'won',\n",
       " 'buy',\n",
       " 'product',\n",
       " 'like',\n",
       " 'this',\n",
       " 'again']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(df.reviewText[0]) # Tokenizing the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store the preprocessed text in a new column\n",
    "review_text =  df.reviewText.apply(gensim.utils.simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [they, look, good, and, stick, good, just, don...\n",
       "1    [these, stickers, work, like, the, review, say...\n",
       "2    [these, are, awesome, and, make, my, phone, lo...\n",
       "3    [item, arrived, in, great, time, and, was, in,...\n",
       "4    [awesome, stays, on, and, looks, great, can, b...\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens next, we utilize a window to walk over the text, it is called \"Context Window\"\n",
    "- By this, we construct the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model implementation is done by word2vec\n",
    "model = gensim.models.Word2Vec(\n",
    "    window = 10, # it means 10 words before target words and 10 after\n",
    "    min_count = 2, # ignore words that occur less than 2 times\n",
    "    workers = 12, # use 4 cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(review_text, progress_per=1000) # progress per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 194439)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs, model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61502199, 83869015)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(review_text,\n",
    "            total_examples = model.corpus_count,\n",
    "            epochs= model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marvell', 0.9481192827224731),\n",
       " ('bsummary', 0.9450608491897583),\n",
       " ('jjyop', 0.9392919540405273),\n",
       " ('hoursled', 0.9313487410545349),\n",
       " ('atranscend', 0.9311302900314331),\n",
       " ('myat', 0.931052565574646),\n",
       " ('hourefficiency', 0.9307219386100769),\n",
       " ('vmg', 0.9303526878356934),\n",
       " ('moremotorola', 0.9290679693222046),\n",
       " ('ouncesshipping', 0.9278916716575623)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"laith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perfect, this is the similarity score! It means the model started to learn the language and know what words are related to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11392945"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1= \"good\", w2= \"learner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7859913"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1= \"great\", w2 = \"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "\n",
    "Download the following dataset and apply the pipeline of word2vec:\n",
    "\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, pandas as pd, numpy as np\n",
    "import gzip, shutil, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to decompress\n",
    "input_file = ... # replace by your new input file\n",
    "output_file = ... # replace by your new output file\n",
    "\n",
    "# Open the .gz file and write the uncompressed data to the output file\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read and analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: Train a word2vec neural network using ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    \n",
    "    @staticmethod    \n",
    "    def load_data(file):\n",
    "        df = ... # Continue\n",
    "        print(\"Data shape is: \",df.shape)\n",
    "        return ...\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocessing(df):\n",
    "        ... = ... # use simple_preprocess from gensim utils\n",
    "        return ...\n",
    "    \n",
    "    @staticmethod\n",
    "    def model_creation(window = 10,\n",
    "                       min_count = 2,\n",
    "                       workers = 4,\n",
    "                       ):\n",
    "        model = ...(    # Use Word2Vec from gensim models\n",
    "            ...,\n",
    "            ...,\n",
    "            ...,\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod    \n",
    "    def model_vocab(model, column, progress_per = 1000):\n",
    "        ... # Use build vocab\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_model(model, column, epochs = model.epochs,\n",
    "                    total_examples = model.corpus_count,):\n",
    "        ... # Use train\n",
    "    \n",
    "    @staticmethod\n",
    "    def model_evaluate_most_similar(model, word):\n",
    "        print(...) # Use most_similar from model wv\n",
    "    \n",
    "    @staticmethod\n",
    "    def model_evaluate_similarity(model, word1, word2):\n",
    "        print(...) # Use similarity from model wv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper:\n",
    "    @staticmethod\n",
    "    def run_pipeline(word = \"bad\",\n",
    "                     word1 = \"great\",\n",
    "                     word2 = \"good\",\n",
    "                     epochs = 5,\n",
    "                     min_count = 2,\n",
    "                     window = 10,\n",
    "                     workers = 4):\n",
    "        ... # Step1\n",
    "        ... # Step2\n",
    "        ... # Step3\n",
    "        ... # Step4\n",
    "        ... # Step5\n",
    "        ... # Step6\n",
    "        ... # Step7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Helper.run_pipeline(...,\n",
    "                    ...,\n",
    "                    ...\n",
    "                    ...,\n",
    "                    ...,\n",
    "                    ...,\n",
    "                    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Section: Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The opposite of hot depends on what you mean by \"hot.\" \n",
      "\n",
      "Here are some possibilities:\n",
      "\n",
      "**Temperature:**\n",
      "* **Cold:**  This is the most common opposite of \"hot\" when referring to temperature. \n",
      "\n",
      "**Other Meanings:**\n",
      "* **Cold:**  Can also be the opposite of \"hot\" in the sense of being emotionally detached or unfriendly.\n",
      "* **Cool:**  Means calm and collected, the opposite of being enthusiastic or passionate.\n",
      "* **Boring:**  Can be the opposite of \"hot\" when describing something exciting or interesting.\n",
      "* **Unpopular:** Can be the opposite of \"hot\" when referring to something trendy or desirable.\n",
      "\n",
      "**Please provide more context!** To give you the most accurate answer, could you tell me how you're using the word \"hot\"? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=\"Replace_This_With_Your_Key\")\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"The opposite of hot is\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Hands-On Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevator Pitch (Seamless Intro)\n",
    "Doc2Vec: Extends Word2Vec by creating embeddings for entire documents or sentences, capturing the meaning of a piece of text rather than just individual words.\n",
    "\n",
    "Difference from Word2Vec: While Word2Vec generates embeddings for words, Doc2Vec creates embeddings for sentences, paragraphs, or full documents, allowing for representation of larger text units.\n",
    "\n",
    "Combination: Word2Vec embeddings can provide context for words within Doc2Vec's larger document embeddings, enhancing overall understanding.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Introduction to Document Embeddings\n",
    "\n",
    "In many NLP applications, it's essential to capture the meaning of whole sentences, paragraphs, or documents rather than individual words alone. **Doc2Vec**, developed by the same researchers behind Word2Vec, provides a way to represent entire pieces of text as dense vectors, preserving semantic meaning in a high-dimensional space.\n",
    "\n",
    "### 2. Use Cases for Doc2Vec\n",
    "\n",
    "Doc2Vec embeddings are beneficial for tasks where understanding entire document semantics is crucial:\n",
    "\n",
    "- **Document Classification**: Categorizing news articles, research papers, or product descriptions.\n",
    "- **Sentiment Analysis**: Determining the sentiment of a whole review, comment, or social media post.\n",
    "- **Recommendation Systems**: Suggesting similar articles, documents, or products based on content similarity.\n",
    "- **Semantic Search**: Matching queries to relevant documents based on semantic similarity.\n",
    "\n",
    "These document embeddings allow models to measure similarity between larger text units, improving clustering, retrieval, and classification in NLP tasks.\n",
    "\n",
    "### 3. How Doc2Vec Works: Models and Training\n",
    "\n",
    "Doc2Vec builds on Word2Vec by adding an extra “document vector” that represents a unique identifier for each document. This vector is learned alongside the word vectors to capture the context and semantics of the entire document. Doc2Vec has two main models:\n",
    "\n",
    "1. **Distributed Memory (DM) Model**: Similar to the Word2Vec CBOW model, this model predicts words based on both surrounding words and a document vector, which provides context for the entire document.\n",
    "2. **Distributed Bag of Words (DBOW) Model**: Similar to the Word2Vec Skip-Gram model, this approach tries to predict context words randomly sampled from the document using only the document vector.\n",
    "\n",
    "### 4. Distributed Memory (DM) Model\n",
    "\n",
    "In the **DM model**, the goal is to predict a target word based on the document vector and the context of surrounding words.\n",
    "\n",
    "#### Objective Function for DM Model\n",
    "\n",
    "For a given document \\( D \\) and target word \\( w_t \\), the model seeks to maximize the log probability of \\( w_t \\) based on surrounding words and \\( D \\):\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T \\log P(w_t | w_{t-c}, \\dots, w_{t+c}, D)\n",
    "$$\n",
    "\n",
    "This approach incorporates document context by learning a unique vector for each document, positioning it closer to similar documents in vector space.\n",
    "\n",
    "### 5. Distributed Bag of Words (DBOW) Model\n",
    "\n",
    "In the **DBOW model**, only the document vector is used to predict words in the document, without considering surrounding context words.\n",
    "\n",
    "#### Objective Function for DBOW Model\n",
    "\n",
    "The DBOW model objective is to maximize the probability of observing a word given the document vector:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^T \\log P(w_t | D)\n",
    "$$\n",
    "\n",
    "The DBOW model is simpler and often faster, making it efficient for training with large datasets, especially when context isn’t crucial.\n",
    "\n",
    "### 6. Optimization Techniques: Negative Sampling and Hierarchical Softmax\n",
    "\n",
    "Similar to Word2Vec, Doc2Vec employs techniques to reduce computational complexity:\n",
    "\n",
    "- **Negative Sampling**: Used to selectively update weights for a few sampled “negative” words rather than the entire vocabulary.\n",
    "  \n",
    "  Negative sampling objective:\n",
    "\n",
    "  $$ \n",
    "  \\log \\sigma(v'_{w_O} \\cdot v_{D}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma(-v'_{w_i} \\cdot v_{D}) \\right]\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - \\( v_{D} \\) is the document vector,\n",
    "  - \\( v'_{w_i} \\) is the output vector for word \\( w_i \\), and\n",
    "  - \\( P_n(w) \\) is the noise distribution for sampling.\n",
    "\n",
    "- **Hierarchical Softmax**: Organizes the vocabulary into a tree structure, reducing complexity when calculating softmax probabilities for large vocabularies.\n",
    "\n",
    "### 7. Limitations of Doc2Vec\n",
    "\n",
    "While Doc2Vec is powerful for generating document embeddings, it has some limitations:\n",
    "\n",
    "- **Context Limitations**: Document embeddings are static, meaning they don't capture dynamic context for words or phrases within each document.\n",
    "- **Training Complexity**: Requires training on large datasets with many documents to learn high-quality embeddings.\n",
    "- **Outdated for Some Applications**: Recent models like Transformers (e.g., BERT) provide contextually adaptive embeddings, which can outperform Doc2Vec in certain tasks.\n",
    "\n",
    "### 8. Example of Doc2Vec in Action\n",
    "\n",
    "Consider the following documents:\n",
    "\n",
    "1. \"The cat sits on the mat.\"\n",
    "2. \"The dog plays with a ball.\"\n",
    "\n",
    "Using Doc2Vec, each document is represented by a unique vector in a high-dimensional space. The model will position the vectors for these sentences closer together if they share similar themes (e.g., both involve animals and objects). This allows for similarity-based retrieval, where related documents appear near each other in vector space.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "## Review: Bag-of-words\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of [n-grams](https://en.wikipedia.org/wiki/N-gram)_\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "## Review: ``Word2Vec`` Model\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "## Introducing: Paragraph Vector\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the [Doc2Vec algorithm](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)_,\n",
    "which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "## Prepare the Training and Test Data\n",
    "\n",
    "For this tutorial, we'll be training our model using the [Lee Background\n",
    "Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf)\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporation’s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter [Lee Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf)\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open # To open the dataset\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published [Paragraph Vector paper](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)_\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a list (accessible via\n",
    "``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.\n",
    "Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,\n",
    "For example, to see how many times ``penalty`` appeared in the training corpus:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'penalty' appeared 4 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "In the usual case, where Gensim installation found a BLAS library for optimized\n",
    "bulk vector operations, this training on this tiny 300 document, ~60k word corpus \n",
    "should take just a few seconds. (More realistic datasets of tens-of-millions\n",
    "of words or more take proportionately longer.) If for some reason a BLAS library \n",
    "isn't available, training uses a fallback approach that takes 60x-120x longer, \n",
    "so even this tiny training will take minutes rather than seconds. (And, in that \n",
    "case, you should also notice a warning in the logging letting you know there's \n",
    "something worth fixing.) So, be sure your installation uses the BLAS-optimized \n",
    "Gensim if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11621892 -0.28047997 -0.15674847  0.22805186  0.08761621 -0.14116004\n",
      " -0.05085545  0.01879746 -0.23177154 -0.1569641   0.18621242 -0.02447974\n",
      "  0.00671212 -0.07895241 -0.1976868  -0.25720164  0.07816963  0.23270506\n",
      "  0.10187422 -0.11422966 -0.03383636 -0.09391351  0.06080697 -0.00323821\n",
      " -0.01044967 -0.07729856 -0.18450584 -0.00332004 -0.15117097 -0.0942106\n",
      "  0.3433787  -0.03551321  0.12097248  0.14798506  0.19807461  0.10160794\n",
      "  0.01346119 -0.27375045 -0.19647905  0.09959803 -0.07022929 -0.02681313\n",
      " -0.05040266 -0.04560855  0.14916752  0.01208238 -0.13709052 -0.07143489\n",
      "  0.07938579 -0.04237286] 50\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector,len(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Model\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 292, 1: 8})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
      "\n",
      "MOST (299, 0.949829638004303): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SECOND-MOST (104, 0.791508674621582): «australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he said»\n",
      "\n",
      "MEDIAN (178, 0.24365578591823578): «year old middle eastern woman is said to be responding well to treatment after being diagnosed with typhoid in temporary holding centre on remote christmas island it could be hours before tests can confirm whether the disease has spread further two of the woman three children boy aged and year old girl have been quarantined with their mother in the christmas island hospital third child remains at the island sports hall where locals say conditions are crowded and hot all detainees on christmas island are being monitored by health team for signs of fever or abdominal pains the key symptoms of typhoid which is spread by contact with contaminated food or water hygiene measures have also been stepped up the western australian health department is briefing medical staff on infection control procedures but locals have expressed concern the disease could spread to the wider community»\n",
      "\n",
      "LEAST (243, -0.09555899351835251): «four afghan factions have reached agreement on an interim cabinet during talks in germany the united nations says the administration which will take over from december will be headed by the royalist anti taliban commander hamed karzai it concludes more than week of negotiations outside bonn and is aimed at restoring peace and stability to the war ravaged country the year old former deputy foreign minister who is currently battling the taliban around the southern city of kandahar is an ally of the exiled afghan king mohammed zahir shah he will serve as chairman of an interim authority that will govern afghanistan for six month period before loya jirga or grand traditional assembly of elders in turn appoints an month transitional government meanwhile united states marines are now reported to have been deployed in eastern afghanistan where opposition forces are closing in on al qaeda soldiers reports from the area say there has been gun battle between the opposition and al qaeda close to the tora bora cave complex where osama bin laden is thought to be hiding in the south of the country american marines are taking part in patrols around the air base they have secured near kandahar but are unlikely to take part in any assault on the city however the chairman of the joint chiefs of staff general richard myers says they are prepared for anything they are prepared for engagements they re robust fighting force and they re absolutely ready to engage if that required he said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (277): «israeli soldiers have shot dead five palestinians in two west bank towns an israeli military source said the soldiers shot four palestinians near jenin when palestinian gunmen opened fire on an army patrol and the troops returned fire another palestinian was killed by israeli soldiers near the west bank city of tulkarem palestinian security source said meanwhile palestinian police have arrested three senior leaders of the hardline hamas group in crackdown that netted more than islamic militants following wave of suicide attacks in israel palestinian security source told the afp news agency hamas official confirmed the arrests of two senior leaders ismail abu shanab and ismail haniya and said police have issued arrest warrants for another two but he refused to name them the security source said more than militants from hamas and the smaller islamic jihad were rounded after yasser arafat palestinian leadership vowed to crackdown on them for wave of anti israeli suicide assaults most of the arrests came after the palestinian leadership declared state of emergency in the palestinian territories giving police sweeping powers to round up militants»\n",
      "\n",
      "Similar Document (208, 0.8926404714584351): «israeli tanks and troops have launched two incursions in the gaza strip near the palestinian self rule city of khan yunis arresting several people and searching houses witnesses say undercover soldiers wearing masks arrived first followed by tanks and additional troops palestinian officials say they were looking for iffam abu daka one of the leaders of the militant democratic front earlier an israeli fighter jet struck three buildings in the palestinian police headquarters in gaza city injuring at least people palestinian officials say two four storey buildings inside the compound were engulfed in flames and destroyed»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (36): «the johannesburg earth summit is set to get under way with the promise that leaders will take action on the environment debt and poverty south african president thabo mbeki speaking at the opening ceremony said out of johannesburg and out of africa must emerge something that takes the world forward but the absence of us president george bush was threatening to overshadow the summit»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
      "\n",
      "MOST (55, 0.6356837153434753): «melbourne weather is one of the question marks hanging over the second test between australia and south africa due to start at the mcg this morning melbourne shaky early summer weather is not yet on the improve this is test cricket traditional day of days and it is receiving melbourne traditional greeting cool and cloudy with possible showers the other questions are over each team pace attack south africa will probably be forced to take huge punt on veteran alan donald while australia is unlikely to punt on brad williams returning andy bichel to test cricket williams was called up to the after jason gillespie was ruled out of the test due to right shoulder injury bichel says he is looking forward to the challenge of call up to the starting side feel that ready to go for this game and looking forward to it bichel said the boxing day test has been good one for me ve got great memories of it so hopefully can repeat those the day is likely to provide conditions any quick bowler would relish bouncy with some seam beckoning the captain who wins the toss to turn his attack loose»\n",
      "\n",
      "MEDIAN (8, 0.22863024473190308): «there has been welcome relief for firefighters in new south wales overnight with milder weather allowing them to strengthen containment lines around the most severe fires but fire authorities are not getting overly optimistic as dry and hot weather is forecast to continue the weather bureau is forecasting temperatures in the high and westerly winds until at least friday which means fire authorities are reluctant to get too excited about last night favourable conditions marks sullivan from the rural fire service says fire fighters are remaining on guard lot of fires that have been burning in the areas around sydney and the north coast and further south have been burning within areas that are known and are contained he said however that not to say that these fires won pose threat given the weather conditions that are coming up over the next few days despite the caution the rural fire service says most of the state fires that threaten property are burning within containment lines greater sydney is ringed by fires to the north west and south two of those flared overnight one at appin in the southern highlands was quickly brought under control another flare up at spencer north of the city is not contained on its north western flank but is not threatening property in the lower blue mountains west of sydney firefighters have spent the night setting up kilometre containment line to protect communities along the great western highway from glenbrook to bulaburra two fires burning near cessnock west of newcastle are still within containment lines in the state north aircraft will this morning check if lightning from large electrical storm overnight has sparked any new fires above grafton aircraft have also been used in the shoalhaven area in the state south to drop incendiary devices that start fire control lines in inaccessible areas the rural fire service commissioner phil koperberg says if fire activity increases hundreds of new year eve fireworks celebrations in new south wales will be cancelled»\n",
      "\n",
      "LEAST (153, -0.14053413271903992): «at least two helicopters have landed near tora bora mountain in eastern afghanistan in what could be the start of raid against al qaeda fighters an afp journalist said the helicopters landed around pm local time am aedt few hours after al qaeda fighters rejected deadline set by afghan militia leaders for them to surrender or face death us warplanes have been bombing the network of caves and tunnels for eight days as part of the hunt for al qaeda leader osama bin laden several witnesses have spoken in recent days of seeing members of us or british special forces near the frontline between the local afghan militia and the followers of bin laden they could not be seen but could be clearly heard as they came into land and strong lights were seen in the same district us bombers and other warplanes staged series of attacks on the al qaeda positions in the white mountains after bin laden fighters failed to surrender all four crew members of us bomber that has crashed in the indian ocean near diego garcia have been rescued us military officials said pentagon spokesman navy captain timothy taylor said initial reports said that all four were aboard the destroyer uss russell which was rushed to the scene after the crash the bomber which usually carries crew of four and is armed with bombs and cruise missiles was engaged in the air war over afghanistan pentagon officials said they had heard about the crash just after am aedt and were unable to say whether the plane was headed to diego garcia or flying from the indian ocean island it is thought the australian arrested in afghanistan for fighting alongside the taliban is from adelaide northern suburbs but the salisbury park family of year old david hicks is remaining silent the president of adelaide islamic society walli hanifi says mr hicks approached him in having just returned from kosovo where he had developed an interest in islam he says mr hicks wanted to know more about the faith but left after few weeks late yesterday afternoon mr hicks salisbury park family told media the australian federal police had told them not to comment local residents confirmed member of the family called mr hicks had travelled to kosovo in recent years and has not been seen for around three years but most including karen white agree they cannot imagine mr hicks fighting for terrorist regime not unless he changed now but when he left here no he wasn he just normal teenage adult boy she said but man known as nick told channel ten he is sure the man detained in afghanistan is his friend david he says in david told him about training in the kosovo liberation army he gone through six weeks basic training how he been in the trenches you know killed few people you know confirmed kills and had few of his mates killed as well the man said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Hands-on Hashing Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizers\n",
    "- Hashing Vectorizer:\n",
    "    - Feature extraction method\n",
    "        - Converting documents into numerical feature vector\n",
    "            - Unlike other vectorizers, it does not store vocabulary, it uses hash function to map tokens to feature indices\n",
    "\n",
    "    - Real-World scenario data size:\n",
    "        - Large-Scale Text Data in Social Media Analysis\n",
    "            - Use Case: Analyzing tweets for sentiment analysis or topic modeling.\n",
    "                - Dataset Size:\n",
    "                    - Number of Tweets: Over 100 million tweets.\n",
    "                    - Average Tweet Length: ~15 words.\n",
    "                    - Total Words: 100 million tweets * 15 words/tweet = 1.5 billion words.\n",
    "                - Vocabulary Size:\n",
    "                    - Due to slang, misspellings, emojis, and hashtags, the unique token count can exceed 10 million.\n",
    "                - Memory Considerations:\n",
    "                    - Storing such a large vocabulary requires significant memory (several gigabytes).\n",
    "                - Why Use Hashing Vectorizer:\n",
    "                    - Memory Efficiency: Doesn't store the vocabulary dictionary.\n",
    "                    - Fixed Feature Space: Can set n_features to a manageable size (e.g., 2^20 = 1,048,576 features).\n",
    "                    - Speed: Faster processing since it avoids the overhead of maintaining a vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "- Let's consider the following three documents:\n",
    "\n",
    "        - Document 1: \"Data science is fun\"\n",
    "        - Document 2: \"Machine learning is powerful\"\n",
    "        - Document 3: \"Data analysis is important\"\n",
    "\n",
    "    - Step 1: Tokenize > First we need to tokenize each document by splitting the text into individual words (tokens).\n",
    "        - Document 1 Tokens: ['Data', 'science', 'is', 'fun']\n",
    "        - Document 2 Tokens: ['Machine', 'learning', 'is', 'powerful']\n",
    "        - Document 3 Tokens: ['Data', 'analysis', 'is', 'important']\n",
    "    \n",
    "    - Step 2: Define Hash Functiom > At second, we define a custom hash function \n",
    "        - Hash Function: For each token, sum the ASCII values of its characters and compute the modulo with the number of features (n_features).\n",
    "        - Hash(token) = (Sum of ASCII codes of characters in token) % n_features\n",
    "\n",
    "    - Step 3: Computing Hash Values > At third, we calculate the hash value for each token.\n",
    "        - Token \"Data\" \n",
    "            - Calculate ASCII Codes: D(68). a(97), t(116), a(97)\n",
    "            - Sum: 68 + 97 + 116 + 97 = 378\n",
    "            - Hash: 378 % 5 = 3\n",
    "        - Token: 'science'\n",
    "            - ASCII Codes: s(115), c(99), i(105), e(101), n(110), c(99), e(101)\n",
    "            - Sum: 730\n",
    "            - Hash: 730 % 5 = 0\n",
    "        - Token: 'is'\n",
    "            - ASCII Codes: i(105), s(115)\n",
    "            - Sum: 220\n",
    "            - Hash: 220 % 5 = 0\n",
    "        - Token: 'fun'\n",
    "            - ASCII Codes: f(102), u(117), n(110)\n",
    "            - Sum: 329\n",
    "            - Hash: 329 % 5 = 4\n",
    "        - Token: 'Machine'\n",
    "            - Sum: 693\n",
    "            - Hash: 693 % 5 = 3\n",
    "        - Token: 'learning'\n",
    "            - Sum: 848\n",
    "            - Hash: 848 % 5 = 3\n",
    "        - Token: 'powerful'\n",
    "            - Sum: 884\n",
    "            - Hash: 884 % 5 = 4\n",
    "        - Token: 'analysis'\n",
    "            - Sum: 868\n",
    "            - Hash: 868 % 5 = 3\n",
    "        - Token: 'important'\n",
    "            - Sum: 990\n",
    "            - Hash: 990 % 5 = 0\n",
    "\n",
    "    - Step 4: Document-Term Matrix > We create a feature vector for each document where each index corresponds to a hashed value. Token & Hash\n",
    "        - Document 1 Vector\n",
    "            - Data : 3\n",
    "            - Science : 0\n",
    "            - is : 0\n",
    "            - fun : 4\n",
    "\n",
    "            Index: 0 1 2 3 4\n",
    "            \n",
    "            Counts: 1 0 0 2 1\n",
    "        \n",
    "        - Document 2 Vector\n",
    "            - Machine: 3\n",
    "            - Learning: 3\n",
    "            - is: 0\n",
    "            - powerful: 4\n",
    "\n",
    "            Index: 0 1 2 3 4\n",
    "            \n",
    "            Counts: 1 0 0 2 1\n",
    "\n",
    "        - Document 3 Vector\n",
    "            - Data: 3\n",
    "            - Analysis: 3\n",
    "            - is: 0\n",
    "            - important: 0\n",
    "\n",
    "- The main drawback is \"COLLISIONS\"\n",
    "    - Notice that some different words have same index hash such as index 0 for science, is and important\n",
    "    - This is called hash collision, where collision are trade-off for memory efficiency, they can introduce noise into the feature represenation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=3, step=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns \n",
    "\n",
    "# docs sample\n",
    "documents = [\n",
    "    \"Data science is fun\",\n",
    "    \"Machine learning is powerful\",\n",
    "    \"Data analysis is important\"\n",
    "]\n",
    "\n",
    "n_features = 5 # Corresponding to number of indeces \n",
    "\n",
    "# [Required ]Custom hash function\n",
    "def simple_hash(token, n_features): # Equivalent to HashVectorizer (by default Murmush hash function)\n",
    "    ascii_sum = sum(ord(char) for char in token)\n",
    "    return ascii_sum % n_features\n",
    "\n",
    "feature_vectors = [] # empty list to store feature vectors\n",
    "\n",
    "# appending documents on iteration\n",
    "for doc in documents:\n",
    "    tokens = doc.split() # Tokenize the document \n",
    "    vector = np.zeros(n_features, dtype=int) # Initialize feature vector\n",
    "    for token in tokens: \n",
    "        index = simple_hash(token, n_features)\n",
    "        vector[index] += 1\n",
    "    feature_vectors.append(vector)\n",
    "\n",
    "# Plotting\n",
    "df = pd.DataFrame(feature_vectors, columns=[f\"Index {i}\" for i in range(n_features)])\n",
    "print(df.index)\n",
    "df.index = [f\"Document {i+1}\" for i in range(len(documents))]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df, annot=True, cmap='YlGnBu')\n",
    "plt.title(\"Document-Term Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guidelines for Choosing Hashing Vectorizer\n",
    "\n",
    "Dataset with Millions of Documents:\n",
    "When the number of documents exceeds 1 million, and you have limited RAM (e.g., 16 GB), Hashing Vectorizer becomes practical.\n",
    "High Vocabulary Turnover:\n",
    "In applications where new words frequently appear (e.g., social media, user-generated content).\n",
    "Limited Memory Resources:\n",
    "When operating in environments with constrained memory (e.g., cloud computing with memory limits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: Transformerrrrrrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Review PDF: Math Behind Transformers](Transformers_Math.pdf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Review PDF: Transformers Code from Scratch](Transformers_Code_Scratch.pdf)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    How the Transformer Processes Data\n",
    "    Input: Tokens are embedded into dense vectors, with positional information added.\n",
    "    Encoding:\n",
    "    Each encoder layer applies self-attention and feed-forward processing.\n",
    "    The output is a contextualized representation of the input sequence.\n",
    "    Decoding:\n",
    "    The decoder generates tokens one by one, attending to its own outputs (masked self-attention) and the encoder outputs (decoder-encoder attention).\n",
    "    Output: The final token probabilities are generated using a softmax layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'architecture.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's narrow down into simplified arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Arch1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Input Embedding\n",
    "    Purpose: Maps discrete tokens (e.g., words or subwords) into dense vector representations.\n",
    "    Details:\n",
    "    Each word or token is converted into a fixed-size vector (e.g., 512-dimensional) using an embedding layer.\n",
    "    These embeddings capture the semantic meaning of tokens (e.g., \"cat\" and \"dog\" might have similar embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Arch2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2. Positional Encoding\n",
    "        Purpose: Since the Transformer doesn’t process sequences in order, positional encoding provides information about the order of tokens in the sequence.\n",
    "        How It Works:\n",
    "        Sine and cosine functions encode positions as continuous, differentiable values.\n",
    "        Positional encodings are added to the input embeddings to create \"positional embeddings.\"\n",
    "        These encodings allow the model to distinguish between \"first word,\" \"second word,\" etc., while still using parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Arch3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Encoder (Nx Layers)\n",
    "    The encoder is composed of multiple identical layers (denoted as 𝑁𝑥) that include:\n",
    "\n",
    "    Multi-Head Self-Attention:\n",
    "\n",
    "    What It Does: Allows each token to attend to every other token in the input sequence.\n",
    "    Key Idea: Captures relationships between tokens to understand context (e.g., \"Paris\" in \"Paris is in France\").\n",
    "    Inputs: Queries, Keys, and Values all come from the same sequence.\n",
    "    Outputs: Weighted combinations of values (contextualized representations).\n",
    "    Add & Norm:\n",
    "\n",
    "    What It Does: Adds a residual connection (input + output of the attention layer) and normalizes the result.\n",
    "    Why: Helps stabilize training and prevents the model from forgetting the original embeddings.\n",
    "    Feed-Forward Network:\n",
    "\n",
    "    What It Does: Applies a fully connected neural network to each token's representation.\n",
    "    Why: Processes the attended information and introduces non-linearity.\n",
    "    Architecture:\n",
    "    Two linear layers with a ReLU activation in between.\n",
    "    Add & Norm (again):\n",
    "\n",
    "    Adds a residual connection and applies layer normalization, just like in the self-attention step.\n",
    "    This process is repeated 𝑁𝑥 \n",
    "    times (e.g., 6 layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Arch4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Attention Mechanisms\n",
    "    Attention mechanisms are central to the Transformer and appear in multiple places in this architecture. Let’s clarify each:\n",
    "\n",
    "    1. Encoder Self-Attention\n",
    "    Goal: Helps each token in the input sequence attend to every other token.\n",
    "    Example:\n",
    "    In the sentence \"The cat is sleeping,\" the word \"cat\" may attend to \"sleeping\" for better context.\n",
    "    2. Masked Decoder Self-Attention\n",
    "    Goal: Ensures the decoder cannot \"see\" future tokens.\n",
    "    Example:\n",
    "    When generating the word \"is,\" the decoder should only consider previous words (e.g., \"The cat\").\n",
    "    3. Decoder-Encoder Attention\n",
    "    Goal: Enables the decoder to use information from the encoder’s output.\n",
    "    Example:\n",
    "    In translation, the decoder attends to the encoder outputs to understand the meaning of the source sentence.\n",
    "\n",
    "    Residual Connections and Layer Normalization\n",
    "    Residual Connections:\n",
    "    Allow the model to \"reuse\" the input information.\n",
    "    Prevent information loss through the layers.\n",
    "    Layer Normalization:\n",
    "    Stabilizes the training process by normalizing the layer outputs.\n",
    "\n",
    "    Feed-Forward Networks\n",
    "    The feed-forward layers allow the model to process attended information further and introduce non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Arch5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Decoder (Nx Layers)\n",
    "    The decoder generates the output sequence step by step. It contains similar components to the encoder but includes extra attention layers:\n",
    "\n",
    "    Masked Multi-Head Self-Attention:\n",
    "\n",
    "    What It Does: Ensures the decoder can only attend to earlier positions in the output sequence (tokens already generated).\n",
    "    Why: Prevents \"cheating\" by looking at future tokens.\n",
    "    How: Uses a mask to block attention to future tokens.\n",
    "    Add & Norm: Same residual and normalization process as in the encoder.\n",
    "\n",
    "    Decoder-Encoder Attention:\n",
    "\n",
    "    What It Does: Allows the decoder to attend to the encoder outputs.\n",
    "    Inputs:\n",
    "    Queries come from the decoder.\n",
    "    Keys and Values come from the encoder outputs.\n",
    "    Why: Enables the decoder to generate output tokens based on the input sequence (e.g., translating \"Paris\" to \"París\").\n",
    "    Add & Norm: Residual connection and normalization.\n",
    "\n",
    "    Feed-Forward Network: Same fully connected neural network as in the encoder.\n",
    "\n",
    "    Add & Norm: Final residual connection and normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Arch6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        5. Output Probabilities\n",
    "        After the decoder processes the input, it produces a final sequence of embeddings.\n",
    "        Softmax Layer:\n",
    "        Converts these embeddings into a probability distribution over the vocabulary (e.g., \"cat,\" \"dog,\" \"is\").\n",
    "        Linear Layer:\n",
    "        Maps the decoder’s final hidden state to the vocabulary size (e.g., 30,000 words)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
