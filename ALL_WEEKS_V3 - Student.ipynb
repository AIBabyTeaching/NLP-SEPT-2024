{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Sept: 2024\n",
    "\n",
    "In this notebook, we have an end-to-end NLP crash course for september 2024\n",
    "\n",
    "Authors:\n",
    "- Eng. Ahmed Métwalli\n",
    "- Eng. Alia Elhefny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "### 1.1 Introduction to NLP\n",
    "\n",
    "Natural Language Processing (NLP) is a multidisciplinary field that combines computer science, linguistics, and artificial intelligence to enable machines to understand, interpret, and generate human language. NLP is ubiquitous in the modern digital world, powering applications such as voice assistants, translation services, and sentiment analysis.\n",
    "\n",
    "#### 1.1.1 What is NLP in the Real World?\n",
    "\n",
    "NLP is applied in various real-world scenarios, including:\n",
    "\n",
    "- **Text Analysis and Summarization**: Automated generation of concise summaries from large documents.\n",
    "- **Sentiment Analysis**: Assessing the emotional tone of texts in social media or customer reviews.\n",
    "- **Machine Translation**: Translating text from one language to another, as seen in Google Translate.\n",
    "- **Chatbots and Virtual Assistants**: Enabling conversational interfaces in applications like Siri, Alexa, and customer support bots.\n",
    "\n",
    "#### 1.1.2 NLP Tasks\n",
    "\n",
    "Key tasks in NLP include:\n",
    "\n",
    "- **Tokenization**: Splitting text into individual words or phrases.\n",
    "- **Named Entity Recognition (NER)**: Identifying and classifying entities like names, places, and organizations.\n",
    "- **Part-of-Speech (POS) Tagging**: Determining the grammatical category (noun, verb, etc.) of each word.\n",
    "- **Dependency Parsing**: Analyzing the grammatical structure of a sentence.\n",
    "- **Text Classification**: Assigning predefined categories to text data, such as spam detection in emails.\n",
    "- **Sentiment Analysis**: Detecting the sentiment or emotion expressed in text.\n",
    "\n",
    "### 1.2 What is Language?\n",
    "\n",
    "Language is a complex system of communication used by humans, comprising various components that convey meaning and facilitate interaction. It consists of several fundamental building blocks:\n",
    "\n",
    "#### 1.2.1 Building Blocks of Language\n",
    "\n",
    "1. **Phonemes**: The smallest units of sound in a language. For example, the word \"cat\" has three phonemes: /k/, /æ/, and /t/.\n",
    "2. **Morphemes**: The smallest units of meaning. \"Unbelievable\" has three morphemes: \"un-\", \"believe\", and \"-able\".\n",
    "3. **Lexemes**: The set of all inflected forms of a single word. For example, \"run\" includes \"runs\", \"ran\", and \"running\".\n",
    "4. **Syntax**: The arrangement of words and phrases to create well-formed sentences. It governs the grammatical structure of language.\n",
    "5. **Context**: The situational background that influences the meaning of words and sentences.\n",
    "\n",
    "### 1.3 Introduction to Approaches to NLP\n",
    "\n",
    "NLP can be approached using various methods, each with its strengths and limitations:\n",
    "\n",
    "#### 1.3.1 Heuristics-Based NLP\n",
    "\n",
    "- Utilizes rule-based methods to process language.\n",
    "- Effective for well-defined, small-scale problems.\n",
    "- Example: Regular expressions for pattern matching in text.\n",
    "\n",
    "#### 1.3.2 Machine Learning for NLP\n",
    "\n",
    "- Uses statistical methods and algorithms to learn from data.\n",
    "- Techniques include supervised and unsupervised learning.\n",
    "- Common algorithms: Naive Bayes, Support Vector Machines (SVM), and decision trees.\n",
    "\n",
    "#### 1.3.3 Deep Learning for NLP\n",
    "\n",
    "- Leverages neural networks, especially deep neural networks, to model complex language patterns.\n",
    "- Significant advancements have been made with architectures like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers.\n",
    "- Applications include language translation, text generation, and question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1: Environment + Hands-on Regex (Heuristic Based)\n",
    "\n",
    "### Environment Preparation:\n",
    "- Download anaconda: https://www.anaconda.com/download/success\n",
    "- Create a new environment called 'NLP_SEPT_2024'\n",
    "    - Set Python version 3.11.x\n",
    "    - Install:\n",
    "        - Notebook\n",
    "        - JupyterLab\n",
    "        - VS Code\n",
    "        - CMD Prompt\n",
    "        - Powershell Prompt\n",
    "    - In Python install basic packages (pip install `package`):\n",
    "        - pandas\n",
    "        - numpy\n",
    "        - matplotlib\n",
    "        - seaborn\n",
    "        - wordcloud # Visualizing the most frequent words in a corpus.\n",
    "        - nltk # Tokenization, POS tagging, stemming, and more.\n",
    "        - spacy # Named Entity Recognition (NER), dependency parsing, and part-of-speech tagging.\n",
    "        - textblob # Sentiment analysis, translation, and language detection.\n",
    "        - gensim # Topic modeling, document similarity, and word embeddings.\n",
    "        - torch # Implementing custom deep learning architectures, fine-tuning models like BERT for NLP tasks.\n",
    "        - transformers # Text classification, translation, question answering, and language generation using pre-trained models.\n",
    "        - sentence-transformers # Text similarity, clustering, and retrieval tasks.\n",
    "        - tensorflow\n",
    "        - keras\n",
    "        - scikit-learn\n",
    "        - chime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Regex Patterns (https://docs.python.org/3/library/re.html):\n",
    "-        . - Matches any character except a newline.\n",
    "-        ^ - Matches the start of the string.\n",
    "-        $ - Matches the end of the string.\n",
    "-        * - Matches 0 or more repetitions of the preceding element.\n",
    "-        + - Matches 1 or more repetitions of the preceding element.\n",
    "-        ? - Matches 0 or 1 repetition of the preceding element.\n",
    "-        {n} - Matches exactly n repetitions of the preceding element.\n",
    "-        {n,} - Matches n or more repetitions of the preceding element.\n",
    "-        {n,m} - Matches between n and m repetitions of the preceding element.\n",
    "-        [] - Matches any one of the enclosed characters.\n",
    "-        | - Alternation; matches either the pattern before or after the |.\n",
    "-        () - Groups multiple patterns into one.\n",
    "-        \\d: Matches any digit (equivalent to [0-9]).\n",
    "-        \\D: Matches any non-digit character.\n",
    "-        \\s: Matches any whitespace character (spaces, tabs, newlines).\n",
    "-        \\S: Matches any non-whitespace character.\n",
    "-        \\b: Matches a word boundary (the position between a word and a non-word character).\n",
    "-        \\B: Matches a non-word boundary.\n",
    "-        \\w: Matches any word character (alphanumeric plus underscore).\n",
    "-        \\W: Matches any non-word character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Practice REGEX: https://regex101.com/r/rsVgaP/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example strings\n",
    "text = \"You should call 911 now. 911 is the emergency number\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all numbers in the text\n",
    "pattern = ...\n",
    "# Use re.findall()\n",
    "matches = ...\n",
    "print(f\"Numbers found: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all numbers with the word '999'\n",
    "pattern = ...\n",
    "# hint use re.sub()\n",
    "replaced_text = ...\n",
    "print(replaced_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emails</th>\n",
       "      <th>username</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john.doe@example.com</td>\n",
       "      <td>john.doe</td>\n",
       "      <td>example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jane_smith@abc.co.uk</td>\n",
       "      <td>jane_smith</td>\n",
       "      <td>abc.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>invalid.email@com</td>\n",
       "      <td>invalid.email</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 emails       username       domain\n",
       "0  john.doe@example.com       john.doe  example.com\n",
       "1  jane_smith@abc.co.uk     jane_smith    abc.co.uk\n",
       "2     invalid.email@com  invalid.email          NaN"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Email \n",
    "data = {'emails': ['john.doe@example.com', 'jane_smith@abc.co.uk', 'invalid.email@com']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Hint: Use df['col'].str.extract()\n",
    "\n",
    "# Extract the username separately\n",
    "# ^: Start of the string.\n",
    "# ([\\w.%+-]+): Captures the username part of the email.\n",
    "# [\\w.%+-]: Matches any word character (letters, digits, underscores), plus the special characters . % + -.\n",
    "# +: One or more of the preceding characters.\n",
    "# @: Matches the literal @ symbol, which is required to separate the username and domain.\n",
    "df['username'] = ...\n",
    "\n",
    "# Extract the domain separately\n",
    "# @: Matches the literal @ symbol, which precedes the domain part.\n",
    "# ([\\w.-]+\\.[a-zA-Z]{2,}): Captures the domain part of the email.\n",
    "# [\\w.-]+: Matches the main domain part, including letters, digits, hyphens, and dots.\n",
    "# \\.[a-zA-Z]{2,}: Matches the top-level domain (TLD) with at least two letters.\n",
    "# $: End of the string.\n",
    "df['domain'] = ...\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: [True, False, True, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Email Validation\n",
    "def validate_email(email):\n",
    "    pattern = ...\n",
    "    # ^(?!.*\\.\\.): Negative lookahead to ensure there are no consecutive dots in the email string.\n",
    "    # [a-zA-Z0-9._%+-]+: Matches the local part (username) of the email. Allows letters, digits, and special characters such as ., _, %, +, -.\n",
    "    # @[a-zA-Z0-9.-]+: Matches the domain part. Allows letters, digits, hyphens, and dots. This pattern allows a single dot but not consecutive dots within the domain part.\n",
    "    # \\.[a-zA-Z]{2,6}$: Matches the TLD with 2 to 6 alphabetic characters, which covers most common TLDs like .com, .org, .museum, etc.\n",
    "    return bool(re.match(pattern, email))\n",
    "# Test the refined function\n",
    "emails = ['test.email@example.com', 'invalid-email@.com', 'name@domain.co', 'test..email@example.com', 'test@domain.c', 'test@domain.toolongtld']\n",
    "results = [validate_email(email) for email in emails]\n",
    "print(f\"Validation Results: {results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results: [True, True, False]\n"
     ]
    }
   ],
   "source": [
    "# Phone Number Validation\n",
    "def validate_phone_number(number):\n",
    "    # Pattern to match common phone number formats\n",
    "    pattern = ...\n",
    "\n",
    "    # ^: Start of the string.\n",
    "    # (\\+\\d{1,3}[-.\\s]?)?: Matches the optional country code part.\n",
    "        # \\+: Matches a literal plus sign '+' at the start, indicating an international code.\n",
    "        # \\d{1,3}: Matches 1 to 3 digits for the country code (e.g., '1' for the US, '44' for the UK).\n",
    "        # [-.\\s]?: Matches an optional separator, which can be a hyphen '-', a dot '.', or a space ' '.\n",
    "        # ?: Makes the entire country code part optional.\n",
    "    # (\\(?\\d{3}\\)?[-.\\s]?)?: Matches the optional area code part.\n",
    "        # \\(?\\d{3}\\)?: Matches 3 digits for the area code, which may or may not be enclosed in parentheses. \n",
    "            # - \\(? : Matches an optional opening parenthesis '('.\n",
    "            # - \\d{3}: Matches exactly 3 digits for the area code.\n",
    "            # - \\)?: Matches an optional closing parenthesis ')'.\n",
    "        # [-.\\s]?: Matches an optional separator (hyphen, dot, or space).\n",
    "            # ?: Makes the entire area code part optional.\n",
    "    # (\\d{3}[-.\\s]?\\d{4}): Matches the main phone number part.\n",
    "        # \\d{3}: Matches exactly 3 digits.\n",
    "        # [-.\\s]?: Matches an optional separator (hyphen, dot, or space).\n",
    "        # \\d{4}: Matches exactly 4 digits for the remaining part of the phone number.\n",
    "    # $: End of the string. Ensures that the pattern matches the entire phone number from start to end.\n",
    "    \n",
    "    return bool(re.fullmatch(pattern, number))\n",
    "\n",
    "\n",
    "# Test the function with a list of phone numbers\n",
    "numbers = ['+1-800-555-5555',  # Valid: Includes country code and separators.\n",
    "           '(123) 456 7890',   # Valid: Area code in parentheses and spaces as separators.\n",
    "           '12345']            # Invalid: Too short to be a valid phone number.\n",
    "\n",
    "# Validate each phone number using the function\n",
    "results = [validate_phone_number(number) for number in numbers]\n",
    "\n",
    "# Display the validation results for each phone number\n",
    "print(f\"Validation Results: {results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URLs: ['https://www.example.com', 'http://blog.example.com']\n"
     ]
    }
   ],
   "source": [
    "# Extract URLs\n",
    "\n",
    "# Extracting URLs from the given text\n",
    "text = 'Visit our website at https://www.example.com or follow us at http://blog.example.com'\n",
    "\n",
    "# Define the pattern to match URLs\n",
    "pattern = ...\n",
    "\n",
    "# https?://: \n",
    "# - https?: Matches the literal 'http' followed optionally by 's'. This means it can match both 'http' and 'https'.\n",
    "# - ://: Matches the literal characters '://', which are required after 'http' or 'https' in a URL.\n",
    "\n",
    "# [a-zA-Z0-9./-]+:\n",
    "# - [a-zA-Z0-9./-]: Character set that matches any of the following characters:\n",
    "#   - a-z: Lowercase English letters.\n",
    "#   - A-Z: Uppercase English letters.\n",
    "#   - 0-9: Digits.\n",
    "#   - . (dot): Matches the literal dot, which is used in domain names and paths.\n",
    "#   - / (forward slash): Matches the literal slash, which is used to separate different parts of the URL.\n",
    "#   - - (hyphen): Matches the literal hyphen, which can be part of domain names or paths.\n",
    "# - +: Quantifier that matches one or more of the preceding characters in the set, ensuring the pattern matches the entire URL.\n",
    "\n",
    "# Hint: Use re.findall()\n",
    "urls = ...\n",
    "\n",
    "print(f\"Extracted URLs: {urls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Dates: ['23/05/1995', '15-04-1992']\n"
     ]
    }
   ],
   "source": [
    "# Extract Birthday\n",
    "# Sample text containing dates\n",
    "text = \"John's birthday is on 23/05/1995 and Mary's is on 15-04-1992.\"\n",
    "\n",
    "# Define the pattern to match date formats\n",
    "pattern = ...\n",
    "\n",
    "# \\b: Matches a word boundary, ensuring that the pattern matches whole numbers and not parts of larger strings.\n",
    "    # - This prevents partial matches like '123' in '123abc'.\n",
    "# \\d{1,2} or \\d{2,4}: \n",
    "# - \\d: Matches any digit from 0 to 9.\n",
    "# - {1,2}: Matches lower or upper digits for the day or month part, allowing for numbers like '3' or '23'.\n",
    "# [-/]: - Matches either a hyphen '-' or a forward slash '/', which are common separators in date formats.\n",
    "\n",
    "\n",
    "# Hint Use re.findall()\n",
    "dates = ...\n",
    "\n",
    "print(f\"Extracted Dates: {dates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello there', 'How are you today', \"Let's learn regex\"]\n"
     ]
    }
   ],
   "source": [
    "# Splitting text to be split into sentences\n",
    "text = \"Hello there! How are you today? Let's learn regex.\"\n",
    "\n",
    "# Define the pattern to split the text into sentences\n",
    "pattern = ...\n",
    "\n",
    "# Explanation of the pattern:\n",
    "# [.!?]:\n",
    "# - [ ]: Square brackets define a character class, which matches any one of the enclosed characters.\n",
    "# - .: Matches a literal period (.) which marks the end of a sentence.\n",
    "# - !: Matches a literal exclamation mark (!) which marks the end of an exclamatory sentence.\n",
    "# - ?: Matches a literal question mark (?) which marks the end of a question.\n",
    "\n",
    "# Hint: Use re.split(pattern,text)\n",
    "sentences = ...\n",
    "\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "print(f\"Sentences: {sentences}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Approaches & Detailed NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Approaches to NLP\n",
    "\n",
    "### Heuristics-Based NLP\n",
    "Heuristic approaches rely on rules and patterns for natural language processing, without relying on data-driven methods. Commonly, regular expressions (regex) are used.\n",
    "\n",
    "**Example:** Named Entity Recognition (NER) using heuristics might involve defining a set of patterns to detect names, locations, or dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ahmed', 'Egypt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Ahmed lives in Egypt\"\n",
    "pattern = r\"...\"  # Simple heuristic for proper nouns\n",
    "# Use \\b and make sure the first letter is capital and then get the rest of the word\n",
    "entities = re.findall(pattern, text)\n",
    "print(entities)  # ['Ahmed', 'Egypt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning for NLP\n",
    "Machine learning approaches involve training models on labeled data. A basic example is using the Naive Bayes algorithm for text classification. One common method for text representation in machine learning is the bag-of-words model, which transforms text into numerical features by counting the occurrences of words.\n",
    "\n",
    "**Equation:**\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "This is the basis of the Naive Bayes classifier, where:\n",
    "- \\( y \\) is the class label (e.g., positive or negative sentiment),\n",
    "- \\( x \\) is the text feature (e.g., a sequence of words),\n",
    "- \\( P(y|x) \\) is the posterior probability of class \\( y \\) given the text feature \\( x \\),\n",
    "- \\( P(x|y) \\) is the likelihood of text feature \\( x \\) given class \\( y \\),\n",
    "- \\( P(y) \\) is the prior probability of class \\( y \\),\n",
    "- \\( P(x) \\) is the probability of the text feature \\( x \\).\n",
    "\n",
    "**Example:** Text classification using Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Utility:\n",
    "    @staticmethod\n",
    "    def vectorize_and_split(texts,labels):\n",
    "        # Step 1: Vectorizing the text data using CountVectorizer (Bag-of-Words model)\n",
    "        vectorizer = ... # Use count vectorizer\n",
    "        X_v = vectorizer.fit_transform(...)  # Transform text into a matrix of token counts\n",
    "        # Step 2: Splitting data into training and test sets\n",
    "        X_train, ..., ..., y_test = train_test_split(..., labels, test_size=0.2, random_state=42)\n",
    "        return X_train,...,...,y_test,vectorizer\n",
    "    @staticmethod\n",
    "    def model_pipeline(X_train,X_test,y_train,y_test):\n",
    "        # Step 3: Initializing the Naive Bayes model (MultinomialNB is suitable for text classification)\n",
    "        model = ... # Use the multinomial NB\n",
    "        # Step 4: Training the model on the training data\n",
    "        ... # Train the model\n",
    "        # Step 5: Making predictions on the test data\n",
    "        y_pred = ... # Predict\n",
    "        # Step 6: Evaluating the model's performance\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return ...\n",
    "    @staticmethod\n",
    "    def model_predict_and_evaluate(model,new_texts,vectorizer):\n",
    "        X_new = vectorizer.transform(...) # transform new data\n",
    "        predictions = model.predict(...) # predict new data \n",
    "        \n",
    "        # Output predictions\n",
    "        for text, pred in zip(new_texts, predictions):\n",
    "            sentiment = \"Positive\" if pred == 1 else \"Negative/Neutral\"\n",
    "            print(f\"Text: '{text}' => Sentiment: {sentiment}\")\n",
    "\n",
    "class Helper:\n",
    "    @staticmethod\n",
    "    def run_pipeline(new_texts,texts,labels):\n",
    "        X_train,...,...,y_test,vectorizer = Utility.vectorize_and_split(texts=texts,labels=labels)\n",
    "        model = Utility.model_pipeline(X_train=X_train,\n",
    "                                       X_test=...,\n",
    "                                       y_train=...,\n",
    "                                       y_test=y_test)\n",
    "        Utility.model_predict_and_evaluate(model,new_texts,vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Predicting the sentiment of new texts\n",
    "new_texts = [\n",
    "    \"Ahmed enjoys coding in Python\",\n",
    "    \"The service was horrible and I am disappointed\",\n",
    "    \"Quantum mechanics is an intriguing field\",\n",
    "    \"I didn't like the food at all, it was tasteless\"\n",
    "]\n",
    "# Sample dataset with texts and corresponding sentiment labels (1: positive, 0: neutral/negative)\n",
    "texts = [\n",
    "    \"Ahmed loves NLP and enjoys learning about machine learning\",\n",
    "    \"Egypt has beautiful landscapes and rich culture\",\n",
    "    \"Heuristics are simple yet effective in problem solving\",\n",
    "    \"Python is a great language for data science and AI\",\n",
    "    \"Natural language processing is fascinating\",\n",
    "    \"I don't like spam emails\",\n",
    "    \"This movie was very boring and too long\",\n",
    "    \"The weather in Egypt is warm and sunny most of the year\",\n",
    "    \"I had a terrible customer service experience\",\n",
    "    \"AI is transforming industries and creating new opportunities\",\n",
    "    \"This is the worst product I have ever bought\",\n",
    "    \"I love exploring new places and experiencing different cultures\",\n",
    "    \"The food at the restaurant was fantastic\",\n",
    "    \"The traffic in Cairo is terrible during rush hours\",\n",
    "    \"I enjoy spending time with my family on the weekends\",\n",
    "    \"The company's customer service was exceptional\",\n",
    "    \"This is an average book with no exciting plot\",\n",
    "    \"I am fascinated by advancements in quantum computing\",\n",
    "    \"The park near my house is always clean and peaceful\",\n",
    "    \"This phone has terrible battery life, I am disappointed\",\n",
    "    # Additional data:\n",
    "    \"The music was beautiful and uplifting\",\n",
    "    \"I am not happy with the slow internet speed\",\n",
    "    \"The staff at the hotel were very polite and helpful\",\n",
    "    \"This software is extremely buggy and crashes often\",\n",
    "    \"I had a great time at the concert\",\n",
    "    \"I am fed up with all the ads in this app\",\n",
    "    \"The customer support was rude and unhelpful\",\n",
    "    \"The car's performance was beyond my expectations\",\n",
    "    \"I hate waiting in long lines\",\n",
    "    \"The book was interesting but too long\",\n",
    "    \"I had a wonderful vacation with my family\",\n",
    "    \"This smartphone is overpriced and not worth the money\",\n",
    "    \"I enjoyed watching the latest movie at the cinema\",\n",
    "    \"The delivery service was fast and efficient\",\n",
    "    \"This laptop is lightweight and has a long battery life\",\n",
    "    \"I am dissatisfied with the product's quality\",\n",
    "    \"The new restaurant has a cozy atmosphere and delicious food\",\n",
    "    \"The airline lost my luggage and I am very upset\",\n",
    "    \"The presentation was informative and well-organized\",\n",
    "    \"The hotel room was dirty and smelled bad\"\n",
    "]\n",
    "# Labels: 1 for positive sentiment, 0 for neutral or negative sentiment\n",
    "labels = [\n",
    "    1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
    "    # Additional labels:\n",
    "    1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n",
      "Text: 'Ahmed enjoys coding in Python' => Sentiment: Positive\n",
      "Text: 'The service was horrible and I am disappointed' => Sentiment: Negative/Neutral\n",
      "Text: 'Quantum mechanics is an intriguing field' => Sentiment: Positive\n",
      "Text: 'I didn't like the food at all, it was tasteless' => Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "Helper.run_pipeline(new_texts=new_texts,\n",
    "                    texts=texts,\n",
    "                    labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminology about last example!!\n",
    "\n",
    "How CountVectorizer Works:\n",
    "CountVectorizer is a simple and effective tool used in Natural Language Processing (NLP) to convert a collection of text documents into a matrix of token counts. It essentially creates a Bag-of-Words (BoW) representation of the text data, where:\n",
    "\n",
    "- Tokenization: It splits the text into words or tokens.\n",
    "- Vocabulary creation: It builds a vocabulary (unique words from the dataset).\n",
    "- Count representation: Each text document is then represented as a vector of word counts, i.e., how many times each word from the vocabulary appears in each document.\n",
    "\n",
    "---\n",
    "\n",
    "Is CountVectorizer the Best Option?\n",
    "While CountVectorizer is effective in many cases, it has limitations:\n",
    "\n",
    "- Ignores word importance: It treats all words equally, meaning frequent words like \"the\" or \"is\" get as much weight as important words, which may not be desirable.\n",
    "- Doesn't consider word context: It doesn't capture the relationship between words (e.g., \"New York\" is treated as two separate words rather than a single entity).\n",
    "- Sparse representations: For large vocabularies, the resulting matrix can be very sparse, with many zero entries.\n",
    "\n",
    "**Other Options and Their Differences:**\n",
    "\n",
    "**TfidfVectorizer** (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "- What it does: It transforms text into a matrix of TF-IDF features. It takes into account the frequency of a word in a document and its frequency across all documents. This way, less frequent but important words are given higher weight compared to common words.\n",
    "- Why it's better: It reduces the importance of frequently occurring words that may not be informative (like \"the\" or \"is\"), leading to better performance for some tasks.\n",
    "- Example: Words that appear in fewer documents will have higher weight, while common words will have a lower weight.\n",
    "\n",
    "\n",
    "**HashingVectorizer**:\n",
    "\n",
    "- What it does: Similar to CountVectorizer, but instead of building a vocabulary, it uses a hashing trick to map terms to indices in a fixed-size vector space.\n",
    "- Why it's different: No need to store a vocabulary, which saves memory. This can be useful when dealing with very large datasets.\n",
    "\n",
    "- Downside: It's not possible to reverse the transformation and map back to the original words since hashing is a one-way process.\n",
    "\n",
    "**Word2Vec or GloVe (Word Embeddings)**:\n",
    "\n",
    "- What it does: Instead of representing words based on their counts, it captures the semantic meaning of words by embedding them into dense vectors. Similar words are mapped close to each other in the vector space.\n",
    "- Why it's better: It captures semantic relationships between words, so words like \"king\" and \"queen\" would have similar vectors, while traditional vectorizers treat them as completely different.\n",
    "- Downside: More complex to train and use, but it can provide more informative features, especially for tasks that benefit from semantic understanding.\n",
    "\n",
    "\n",
    "**BERT, GPT, and other transformer-based models**:\n",
    "\n",
    "- What they do: These models use deep learning to generate context-aware word embeddings. Each word is represented based on the context in which it appears.\n",
    "- Why it's better: Unlike CountVectorizer or TfidfVectorizer, they capture the meaning of a word depending on its surrounding words, making them much more powerful for complex NLP tasks like sentiment analysis, text classification, and question answering.\n",
    "Downside: Computationally expensive and require large datasets for training. They also need more processing power compared to simpler vectorizers.\n",
    "\n",
    "When to Use Which?\n",
    "- For simple tasks with limited data: CountVectorizer or - TfidfVectorizer are usually sufficient. TfidfVectorizer tends to perform better in cases where the frequency of common words needs to be downplayed.\n",
    "\n",
    "- For large-scale or production tasks: HashingVectorizer is useful when dealing with very large datasets, especially when memory is a concern.\n",
    "\n",
    "- For advanced NLP tasks requiring semantic understanding: Word embeddings (Word2Vec, GloVe) or transformer-based models like BERT are more powerful, capturing context and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for NLP\n",
    "Deep learning approaches, such as Recurrent Neural Networks (RNNs) and Transformers, have revolutionized NLP by modeling complex dependencies in text data. RNNs use sequential data with hidden states, while Transformers, like BERT, use attention mechanisms for parallel processing.\n",
    "\n",
    "**Equation:** \n",
    "The attention mechanism in Transformers is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Q \\) is the **query**,\n",
    "- \\( K \\) is the **key**,\n",
    "- \\( V \\) is the **value**,\n",
    "\n",
    "and \\( d_k \\) is the dimension of the key.\n",
    "\n",
    "**Example:** Using Huggingface's BERT for text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ...\n",
    "\"\"\"Imports the pipeline function from the Hugging Face Transformers library. \n",
    "The pipeline is a high-level abstraction that allows you to easily use state-of-the-art NLP models without worrying about the underlying complexity.\"\"\"\n",
    "\n",
    "\n",
    "classifier = ...(...) # use sentiment analysis from pipeline\n",
    "\"\"\" Uses Bidirectional Encoder Representations from Transformers (BERT).\n",
    "This should creates a pre-configured pipeline for sentiment analysis.\n",
    "By default, Hugging Face uses a pre-trained model from the BERT family, specifically DistilBERT, which has been fine-tuned for sentiment analysis. It will automatically download the required model weights and tokenizer when you first run the code.\"\"\"\n",
    "\n",
    "\n",
    "result = ...(\"Ahmed enjoys studying NLP\")\n",
    "\"\"\"Text tokenization: Behind the scenes, the input text is first tokenized, which means it's split into smaller units (tokens) that the model can process.\n",
    "Model prediction: The pre-trained model then processes these tokens and predicts whether the text expresses a positive or negative sentiment. Since the text is about \"enjoying\" something, the model will likely predict positive sentiment.\n",
    "Post-processing: The result is then converted back into a human-readable label, which in this case is either \"POSITIVE\" or \"NEGATIVE\", along with a confidence score.\"\"\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Pipeline in Detail\n",
    "\n",
    "### 1. Data Acquisition\n",
    "The first step in any NLP project is acquiring the data. This can come from web scraping, APIs, or existing datasets.\n",
    "\n",
    "**Example:** Load a dataset (e.g., IMDB movie reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "df = pd.DataFrame({'text': data.data, 'label': data.target})\n",
    "\n",
    "# Take a look at the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Cleaning\n",
    "Text cleaning involves removing noise such as punctuation, numbers, stopwords, and converting text to lowercase.\n",
    "\n",
    "**Example:** Simple text cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Metwalli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\"\"\"nltk.corpus.stopwords: provides a corpus of stopwords (common words like \"the\", \"is\", \"in\", etc.) for various languages. Stopwords are often removed in text preprocessing since they don't contribute much to the meaning of the text for certain tasks like classification.\"\"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Purpose: Text cleaning and normalization are crucial steps before further processing or analysis, especially in Natural Language Processing (NLP). This ensures the text is free from unnecessary noise like digits, punctuation, and stopwords, and that it’s all in a consistent format (lowercase).\"\"\"\n",
    "    #Hint: Use .sub() from regex and remove numbers and punctuation\n",
    "    text = ... # This removes all numbers from the text \n",
    "    text = ... # regular expression that matches any character that is not a word (\\w) or whitespace (\\s), effectively removing punctuation.\n",
    "    text = ... #Converts the text to lowercase to standardize it.\n",
    "    text = ' '.join([word for word in text.split() if word not in ...('english')]) # Remove stop words\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-processing\n",
    "Pre-processing includes tokenization, lemmatization, and stemming.\n",
    "\n",
    "**Equation:** For word embeddings like TF-IDF, the formula is:\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\log \\left( \\frac{N}{\\text{df}(t)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **tf(t, d)** is the term frequency of term \\( t \\) in document \\( d \\),\n",
    "- **df(t)** is the document frequency of term \\( t \\) across all documents,\n",
    "- **N** is the total number of documents.\n",
    "\n",
    "**Example:** Tokenization and stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Metwalli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Metwalli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#We import these functions because we are going to tokenize the text (split it into individual words) and then apply stemming to convert words to their root form for further processing.\n",
    "\n",
    "ps = ... #This creates an instance of the PorterStemmer, which will be used to stem each tokenized word.\n",
    "\n",
    "df['tokens'] = df['cleaned_text'].apply(...) # Use lambda function\n",
    "df['stemmed_tokens'] = df['tokens'].apply(lambda x: [... for word in x]) # use stem(words) from ps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering\n",
    "This involves representing text as numeric features. Popular techniques include Bag-of-Words, TF-IDF, and Word Embeddings (Word2Vec, GloVe).\n",
    "\n",
    "**Example:** TF-IDF vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\"\"\"The TfidfVectorizer is used to convert text into a numerical form that machine learning models can understand. The vectorizer automatically handles tokenization and computes the TF-IDF score for each word.\"\"\"\n",
    "tfidf = ... # use an instance of tfidf\n",
    "X = ... # use Fit & transform in tfidf on the cleaned_text from the df\n",
    "\n",
    "\"\"\"it will:\n",
    "Tokenize the text (i.e., split it into words).\n",
    "Compute the term frequency for each word in each document.\n",
    "Compute the inverse document frequency (IDF) across the corpus.\n",
    "Multiply the TF and IDF values to get the TF-IDF score for each word in each document.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modeling\n",
    "You can now apply machine learning or deep learning models like Logistic Regression, SVM, or neural networks.\n",
    "\n",
    "**Example:** Logistic Regression for text classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation\n",
    "Model evaluation metrics include accuracy, precision, recall, and F1 score.\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP** = true positives,\n",
    "- **FP** = false positives,\n",
    "- **FN** = false negatives.\n",
    "\n",
    "**Example:** Evaluating model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       480\n",
      "           1       0.96      0.97      0.97       584\n",
      "           2       0.95      0.96      0.95       591\n",
      "           3       0.93      0.95      0.94       590\n",
      "           4       0.98      0.98      0.98       578\n",
      "           5       0.98      0.98      0.98       593\n",
      "           6       0.94      0.96      0.95       585\n",
      "           7       0.98      0.98      0.98       594\n",
      "           8       1.00      0.99      0.99       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      0.98      0.99       595\n",
      "          12       0.98      0.97      0.98       591\n",
      "          13       1.00      0.99      0.99       594\n",
      "          14       1.00      0.99      1.00       593\n",
      "          15       0.96      1.00      0.98       599\n",
      "          16       0.98      0.99      0.99       546\n",
      "          17       1.00      0.99      1.00       564\n",
      "          18       0.99      0.98      0.98       465\n",
      "          19       0.99      0.92      0.95       377\n",
      "\n",
      "    accuracy                           0.98     11314\n",
      "   macro avg       0.98      0.98      0.98     11314\n",
      "weighted avg       0.98      0.98      0.98     11314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(classification_report(df['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Deployment\n",
    "Deploying the model can involve using platforms like FastAPI or Flask for serving predictions.\n",
    "\n",
    "**Example:** FastAPI skeleton for deploying an NLP model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "\n",
    "app = FastAPI()\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "def predict(text: str):\n",
    "    return {\"prediction\": model.predict([text])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Monitoring and Model Updating\n",
    "Monitoring involves tracking model performance post-deployment and updating the model when performance degrades.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitocluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
